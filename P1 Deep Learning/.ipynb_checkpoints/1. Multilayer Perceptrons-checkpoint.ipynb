{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dc51cca",
   "metadata": {},
   "source": [
    "# Multilayer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c95c2f",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center;\">\n",
    "    <img src=\"../imgs/MLP.jpg\" alt=\"Your Image\" width=\"600\" style=\"margin-right: 20px;\">\n",
    "    <div>\n",
    "        <p>The Multi-Layer Perceptron (MLP) serves as a fundamental building block in neural network architectures, offering a straightforward yet powerful framework for various machine learning tasks. A typical MLP comprises an input layer, hidden layer(s), and output layer, all densely interconnected—a characteristic known as fully connected.</p>\n",
    "        <p>MLPs excel in classification tasks, making them essential tools in machine learning. They adeptly discern patterns and relationships in complex datasets, particularly in tasks like image recognition and natural language processing. Driven by the backpropagation algorithm, MLPs iteratively adjust parameters to optimize performance on a given task.</p>\n",
    "        <p>Join us as we embark on implementing an MLP from scratch, exploring neural network architecture and diving into the intricacies of training and testing. Our destination? The renowned MNIST dataset—a staple in machine learning benchmarks. Through this journey, we aim to understand the inner workings of MLPs and witness their prowess in digit recognition!</p>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52614be2",
   "metadata": {},
   "source": [
    "## MLP code implementation\n",
    "\n",
    "Let's first define a simple MLP with only one hidden layer.\n",
    "\n",
    "This multilayer perceptron (MLP) model consists of three linear layers (fully connected layers). And during the forward propagation process, data passes through each linear layer, followed by the application of the ReLU activation function to introduce non-linearity into the model. Finally, the predicted output `y_hat` is returned from the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de3b0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09762e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_fc = nn.Linear(input_size, 256)\n",
    "        self.hidden_fc_1 = nn.Linear(256, 128)\n",
    "        self.output_fc = nn.Linear(128, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        x = F.relu(self.input_fc(x))\n",
    "        x = F.relu(self.hidden_fc_1(x))\n",
    "        y_hat = self.output_fc(x)\n",
    "        \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7dd89f",
   "metadata": {},
   "source": [
    "## Train&Test MLP on MNIST\n",
    "\n",
    "The MNIST dataset is a widely-used benchmark dataset in the field of machine learning. It consists of a large collection of handwritten digits, ranging from 0 to 9. Each digit image is a grayscale image with a size of 28x28 pixels. MNIST is often used for training and testing various machine learning algorithms, particularly for tasks like image classification and digit recognition. Its simplicity and accessibility make it an ideal starting point for learning and experimenting with machine learning techniques.\n",
    "\n",
    "In this chapter, we will manually finish dataset download, data normalization, data augmentation and train the model to familiarize ourselves with the training process of the entire model on MNIST. But in the following chapters, we will directly call the classes defined based on this process.\n",
    "\n",
    "And also, we don't focus on the result of the train or test. The important thing is that you know how to structure an MLP and train it. You may see a accuracy which is not the best on the validation and test data because I didn't spend a lot of time finding suitable parameters, but you can try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "373cac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a241647e",
   "metadata": {},
   "source": [
    "### Set the Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2ae0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d034a8d",
   "metadata": {},
   "source": [
    "### Download and allocate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e05cf3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numble of train examples:54000\n",
      "The numble of valid examples:6000\n",
      "The numble of test examples:10000\n"
     ]
    }
   ],
   "source": [
    "ROOT = '.data'\n",
    "train_valid_data = torchvision.datasets.MNIST(root=ROOT, train=True, download=True)\n",
    "test_data = torchvision.datasets.MNIST(root=ROOT, train=False, download=False)\n",
    "VALID_RATE = 0.1\n",
    "train_data, valid_data = torch.utils.data.random_split(train_valid_data, \n",
    "                                           [int(len(train_valid_data)*(1-VALID_RATE)),\n",
    "                                            int(len(train_valid_data)* VALID_RATE)])\n",
    "\n",
    "print(f\"The numble of train examples:{len(train_data)}\")\n",
    "print(f\"The numble of valid examples:{len(valid_data)}\")\n",
    "print(f\"The numble of test examples:{len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b95bf7-2ce3-4128-9c3d-8735ebf702be",
   "metadata": {},
   "source": [
    "### Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "527dfb6e-c759-4514-96e9-bc47b422741f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated mean:0.13066047430038452\n",
      "Calculated std:0.30810779333114624\n"
     ]
    }
   ],
   "source": [
    "mean = train_valid_data.data.float().mean()/255.0\n",
    "std = train_valid_data.data.float().std()/255.0\n",
    "\n",
    "print(f\"Calculated mean:{mean}\")\n",
    "print(f\"Calculated std:{std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f869e7e6",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0abb21a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomRotation(5, fill=(0,)),\n",
    "    torchvision.transforms.RandomCrop(28, padding=2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[mean], std=[std])\n",
    "])\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[mean], std=[std])\n",
    "])\n",
    "\n",
    "train_data.dataset.transform = train_transforms\n",
    "valid_data.dataset.transform = test_transforms\n",
    "test_data.transform = test_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c4c5a",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "200969b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e27b99",
   "metadata": {},
   "source": [
    "### Load model and Start to train&valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dd0ca0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Now start training -----\n",
      "----- Start training with epoch 1 -----\n",
      "This epoch training time is 9.224442958831787 seconds.\n",
      "Training step: 211, Average Loss: 0.2696020949755517\n",
      "The total Loss on valid dataset: 0.12312737014144659\n",
      "The Accuracy on valid dataset: 96.12%\n",
      "----- Start training with epoch 2 -----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m epoch_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m     25\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\bdl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\bdl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\bdl\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\bdl\\lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\bdl\\lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\bdl\\lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\bdl\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\bdl\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\bdl\\lib\\site-packages\\torchvision\\transforms\\functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MLP(input_size=28*28, output_size=10)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "if torch.cuda.is_available():\n",
    "    loss_fn = loss_fn.cuda()\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scaler = GradScaler()\n",
    "\n",
    "total_train_step = 0\n",
    "total_valid_step = 0\n",
    "epoch = 10\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"----- Now start training -----\")\n",
    "for i in range(epoch):\n",
    "    print(f\"----- Start training with epoch {i + 1} -----\")\n",
    "    epoch_train_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for data in train_dataloader:\n",
    "        inputs, targets = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        optimizer.zero_grad()\n",
    "        scaler.update()\n",
    "\n",
    "        total_train_step += 1\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    end_time = time.time()\n",
    "    avg_epoch_train_loss = epoch_train_loss / len(train_dataloader)\n",
    "    print(f\"This epoch training time is {end_time - start_time} seconds.\")\n",
    "    print(f\"Training step: {total_train_step}, Average Loss: {avg_epoch_train_loss}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.eval()\n",
    "    total_valid_loss = 0\n",
    "    total_accuracy = 0\n",
    "    num_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_dataloader:\n",
    "            inputs, targets = data\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_valid_loss += loss.item()\n",
    "            accuracy = (outputs.argmax(1) == targets).sum().item()\n",
    "            total_accuracy += accuracy\n",
    "            num_samples += len(inputs)\n",
    "\n",
    "    avg_valid_loss = total_valid_loss / len(valid_dataloader)\n",
    "    avg_accuracy = total_accuracy / num_samples * 100\n",
    "    print(f\"The total Loss on valid dataset: {avg_valid_loss}\")\n",
    "    print(f\"The Accuracy on valid dataset: {avg_accuracy:.2f}%\")\n",
    "    total_valid_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c898c22",
   "metadata": {},
   "source": [
    "### Go testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4c71a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Test Results -----\n",
      "The total Loss on test dataset: 0.14802024265227373\n",
      "The Accuracy on test dataset: 96.84%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "total_test_loss = 0\n",
    "total_test_accuracy = 0\n",
    "num_samples = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        inputs, targets = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        total_test_loss += loss.item()\n",
    "        accuracy = (outputs.argmax(1) == targets).sum().item()\n",
    "        total_test_accuracy += accuracy\n",
    "        num_samples += len(inputs)\n",
    "\n",
    "avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "avg_test_accuracy = total_test_accuracy / num_samples * 100\n",
    "\n",
    "print(\"----- Test Results -----\")\n",
    "print(f\"The total Loss on test dataset: {avg_test_loss}\")\n",
    "print(f\"The Accuracy on test dataset: {avg_test_accuracy:.2f}%\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdl",
   "language": "python",
   "name": "bdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
