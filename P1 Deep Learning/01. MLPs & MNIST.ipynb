{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dc51cca",
   "metadata": {},
   "source": [
    "# Multilayer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c95c2f",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center;\">\n",
    "    <img src=\"../imgs/MLP.jpg\" alt=\"Your Image\" width=\"600\" style=\"margin-right: 20px;\">\n",
    "    <div>\n",
    "        <p>The Multi-Layer Perceptron (MLP) serves as a fundamental building block in neural network architectures, offering a straightforward yet powerful framework for various machine learning tasks. A typical MLP comprises an input layer, hidden layer(s), and output layer, all densely interconnected—a characteristic known as fully connected.</p>\n",
    "        <p>MLPs excel in classification tasks, making them essential tools in machine learning. They adeptly discern patterns and relationships in complex datasets, particularly in tasks like image recognition and natural language processing. Driven by the backpropagation algorithm, MLPs iteratively adjust parameters to optimize performance on a given task.</p>\n",
    "        <p>Join us as we embark on implementing an MLP from scratch, exploring neural network architecture and diving into the intricacies of training and testing. Our destination? The renowned MNIST dataset—a staple in machine learning benchmarks. Through this journey, we aim to understand the inner workings of MLPs and witness their prowess in digit recognition!</p>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52614be2",
   "metadata": {},
   "source": [
    "## MLP code implementation\n",
    "\n",
    "Let's first define a simple MLP with only one hidden layer.\n",
    "\n",
    "This multilayer perceptron (MLP) model consists of three linear layers (fully connected layers). And during the forward propagation process, data passes through each linear layer, followed by the application of the ReLU activation function to introduce non-linearity into the model. Finally, the predicted output `y_hat` is returned from the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09762e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_fc = nn.Linear(input_size, 256)\n",
    "        self.hidden_fc_1 = nn.Linear(256, 128)\n",
    "        self.output_fc = nn.Linear(128, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        x = F.relu(self.input_fc(x))\n",
    "        x = F.relu(self.hidden_fc_1(x))\n",
    "        y_hat = self.output_fc(x)\n",
    "        \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7dd89f",
   "metadata": {},
   "source": [
    "## Train&Test MLP on MNIST\n",
    "\n",
    "The MNIST dataset is a widely-used benchmark dataset in the field of machine learning. It consists of a large collection of handwritten digits, ranging from 0 to 9. Each digit image is a grayscale image with a size of 28x28 pixels. MNIST is often used for training and testing various machine learning algorithms, particularly for tasks like image classification and digit recognition. Its simplicity and accessibility make it an ideal starting point for learning and experimenting with machine learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "373cac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a241647e",
   "metadata": {},
   "source": [
    "### Set the Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ae0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d034a8d",
   "metadata": {},
   "source": [
    "### Download and allocate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e05cf3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to .data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 9912422/9912422 [00:02<00:00, 3305933.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .data\\MNIST\\raw\\train-images-idx3-ubyte.gz to .data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to .data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 134341.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to .data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to .data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1648877/1648877 [00:01<00:00, 1079676.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to .data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to .data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<00:00, 1512667.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to .data\\MNIST\\raw\n",
      "\n",
      "The numble of train examples:54000\n",
      "The numble of valid examples:6000\n",
      "The numble of test examples:10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ROOT = '.data'\n",
    "train_valid_data = torchvision.datasets.MNIST(root=ROOT, train=True, download=True)\n",
    "test_data = torchvision.datasets.MNIST(root=ROOT, train=False, download=False)\n",
    "VALID_RATE = 0.1\n",
    "train_data, valid_data = torch.utils.data.random_split(train_valid_data, \n",
    "                                           [int(len(train_valid_data)*(1-VALID_RATE)),\n",
    "                                            int(len(train_valid_data)* VALID_RATE)])\n",
    "\n",
    "print(f\"The numble of train examples:{len(train_data)}\")\n",
    "print(f\"The numble of valid examples:{len(valid_data)}\")\n",
    "print(f\"The numble of test examples:{len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b95bf7-2ce3-4128-9c3d-8735ebf702be",
   "metadata": {},
   "source": [
    "### Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "527dfb6e-c759-4514-96e9-bc47b422741f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated mean:0.13066047430038452\n",
      "Calculated std:0.30810779333114624\n"
     ]
    }
   ],
   "source": [
    "mean = train_valid_data.data.float().mean()/255.0\n",
    "std = train_valid_data.data.float().std()/255.0\n",
    "\n",
    "print(f\"Calculated mean:{mean}\")\n",
    "print(f\"Calculated std:{std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f869e7e6",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0abb21a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "train_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomRotation(5, fill=(0,)),\n",
    "    torchvision.transforms.RandomCrop(28, padding=2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[mean], std=[std])\n",
    "])\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[mean], std=[std])\n",
    "])\n",
    "\n",
    "train_data.dataset.transform = train_transforms\n",
    "valid_data.dataset.transform = test_transforms\n",
    "test_data.transform = test_transforms\n",
    "\n",
    "print(train_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c4c5a",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "200969b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e27b99",
   "metadata": {},
   "source": [
    "### Load model and Start to train&valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dd0ca0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Now start training -----\n",
      "----- Start training with epoch 1 -----\n",
      "This epoch training time is 8.421887397766113 seconds.\n",
      "Training step: 211, Average Loss: 0.2805302936177683\n",
      "The total Loss on valid dataset: 0.18387826159596443\n",
      "The Accuracy on valid dataset: 94.57%\n",
      "----- Start training with epoch 2 -----\n",
      "This epoch training time is 9.451736450195312 seconds.\n",
      "Training step: 422, Average Loss: 0.12983831309523627\n",
      "The total Loss on valid dataset: 0.1489059990271926\n",
      "The Accuracy on valid dataset: 95.53%\n",
      "----- Start training with epoch 3 -----\n",
      "This epoch training time is 9.430496215820312 seconds.\n",
      "Training step: 633, Average Loss: 0.1062478459400447\n",
      "The total Loss on valid dataset: 0.1367642773936192\n",
      "The Accuracy on valid dataset: 96.10%\n",
      "----- Start training with epoch 4 -----\n",
      "This epoch training time is 9.696455717086792 seconds.\n",
      "Training step: 844, Average Loss: 0.08660069806280576\n",
      "The total Loss on valid dataset: 0.14670784818008542\n",
      "The Accuracy on valid dataset: 96.02%\n",
      "----- Start training with epoch 5 -----\n",
      "This epoch training time is 9.499501466751099 seconds.\n",
      "Training step: 1055, Average Loss: 0.08959114072199964\n",
      "The total Loss on valid dataset: 0.14856021773690978\n",
      "The Accuracy on valid dataset: 96.28%\n",
      "----- Start training with epoch 6 -----\n",
      "This epoch training time is 9.548604249954224 seconds.\n",
      "Training step: 1266, Average Loss: 0.08256391456165302\n",
      "The total Loss on valid dataset: 0.1538230205575625\n",
      "The Accuracy on valid dataset: 96.23%\n",
      "----- Start training with epoch 7 -----\n",
      "This epoch training time is 9.396120071411133 seconds.\n",
      "Training step: 1477, Average Loss: 0.07838411226656765\n",
      "The total Loss on valid dataset: 0.16724998348702988\n",
      "The Accuracy on valid dataset: 96.52%\n",
      "----- Start training with epoch 8 -----\n",
      "This epoch training time is 9.523454666137695 seconds.\n",
      "Training step: 1688, Average Loss: 0.06865078058111442\n",
      "The total Loss on valid dataset: 0.1464264364913106\n",
      "The Accuracy on valid dataset: 96.22%\n",
      "----- Start training with epoch 9 -----\n",
      "This epoch training time is 9.498873233795166 seconds.\n",
      "Training step: 1899, Average Loss: 0.07546364939749524\n",
      "The total Loss on valid dataset: 0.19788919699688753\n",
      "The Accuracy on valid dataset: 96.38%\n",
      "----- Start training with epoch 10 -----\n",
      "This epoch training time is 9.399989604949951 seconds.\n",
      "Training step: 2110, Average Loss: 0.0710615690982949\n",
      "The total Loss on valid dataset: 0.19177484946946302\n",
      "The Accuracy on valid dataset: 96.65%\n"
     ]
    }
   ],
   "source": [
    "model = MLP(input_size=28*28, output_size=10)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "if torch.cuda.is_available():\n",
    "    loss_fn = loss_fn.cuda()\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scaler = GradScaler()\n",
    "\n",
    "total_train_step = 0\n",
    "total_valid_step = 0\n",
    "epoch = 10\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"----- Now start training -----\")\n",
    "for i in range(epoch):\n",
    "    print(f\"----- Start training with epoch {i + 1} -----\")\n",
    "    epoch_train_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for data in train_dataloader:\n",
    "        inputs, targets = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        optimizer.zero_grad()\n",
    "        scaler.update()\n",
    "\n",
    "        total_train_step += 1\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    end_time = time.time()\n",
    "    avg_epoch_train_loss = epoch_train_loss / len(train_dataloader)\n",
    "    print(f\"This epoch training time is {end_time - start_time} seconds.\")\n",
    "    print(f\"Training step: {total_train_step}, Average Loss: {avg_epoch_train_loss}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.eval()\n",
    "    total_valid_loss = 0\n",
    "    total_accuracy = 0\n",
    "    num_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_dataloader:\n",
    "            inputs, targets = data\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_valid_loss += loss.item()\n",
    "            accuracy = (outputs.argmax(1) == targets).sum().item()\n",
    "            total_accuracy += accuracy\n",
    "            num_samples += len(inputs)\n",
    "\n",
    "    avg_valid_loss = total_valid_loss / len(valid_dataloader)\n",
    "    avg_accuracy = total_accuracy / num_samples * 100\n",
    "    print(f\"The total Loss on valid dataset: {avg_valid_loss}\")\n",
    "    print(f\"The Accuracy on valid dataset: {avg_accuracy:.2f}%\")\n",
    "    total_valid_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c898c22",
   "metadata": {},
   "source": [
    "### Go testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4c71a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Test Results -----\n",
      "The total Loss on test dataset: 0.15674970799518634\n",
      "The Accuracy on test dataset: 96.86%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "total_test_loss = 0\n",
    "total_test_accuracy = 0\n",
    "num_samples = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        inputs, targets = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        total_test_loss += loss.item()\n",
    "        accuracy = (outputs.argmax(1) == targets).sum().item()\n",
    "        total_test_accuracy += accuracy\n",
    "        num_samples += len(inputs)\n",
    "\n",
    "avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "avg_test_accuracy = total_test_accuracy / num_samples * 100\n",
    "\n",
    "print(\"----- Test Results -----\")\n",
    "print(f\"The total Loss on test dataset: {avg_test_loss}\")\n",
    "print(f\"The Accuracy on test dataset: {avg_test_accuracy:.2f}%\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdl",
   "language": "python",
   "name": "bdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
