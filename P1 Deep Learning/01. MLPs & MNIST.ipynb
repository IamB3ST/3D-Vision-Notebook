{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dc51cca",
   "metadata": {},
   "source": [
    "# Multilayer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c95c2f",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center;\">\n",
    "    <img src=\"../imgs/MLP.jpg\" alt=\"Your Image\" width=\"600\" style=\"margin-right: 20px;\">\n",
    "    <div>\n",
    "        <p>The Multi-Layer Perceptron (MLP) serves as a fundamental building block in neural network architectures, offering a straightforward yet powerful framework for various machine learning tasks. A typical MLP comprises an input layer, hidden layer(s), and output layer, all densely interconnected—a characteristic known as fully connected.</p>\n",
    "        <p>MLPs excel in classification tasks, making them essential tools in machine learning. They adeptly discern patterns and relationships in complex datasets, particularly in tasks like image recognition and natural language processing. Driven by the backpropagation algorithm, MLPs iteratively adjust parameters to optimize performance on a given task.</p>\n",
    "        <p>Join us as we embark on implementing an MLP from scratch, exploring neural network architecture and diving into the intricacies of training and testing. Our destination? The renowned MNIST dataset—a staple in machine learning benchmarks. Through this journey, we aim to understand the inner workings of MLPs and witness their prowess in digit recognition!</p>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52614be2",
   "metadata": {},
   "source": [
    "## MLP code implementation\n",
    "\n",
    "Let's first define a simple MLP with only one hidden layer.\n",
    "\n",
    "This multilayer perceptron (MLP) model consists of three linear layers (fully connected layers). And during the forward propagation process, data passes through each linear layer, followed by the application of the ReLU activation function to introduce non-linearity into the model. Finally, the predicted output `y_hat` is returned from the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09762e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\bdl\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Anaconda\\envs\\bdl\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "D:\\Anaconda\\envs\\bdl\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_fc = nn.Linear(input_size, 256)\n",
    "        self.hidden_fc_1 = nn.Linear(256, 128)\n",
    "        self.output_fc = nn.Linear(128, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        x = F.relu(self.input_fc(x))\n",
    "        x = F.relu(self.hidden_fc_1(x))\n",
    "        y_hat = self.output_fc(x)\n",
    "        \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7dd89f",
   "metadata": {},
   "source": [
    "## Train&Test MLP on MNIST\n",
    "\n",
    "The MNIST dataset is a widely-used benchmark dataset in the field of machine learning. It consists of a large collection of handwritten digits, ranging from 0 to 9. Each digit image is a grayscale image with a size of 28x28 pixels. MNIST is often used for training and testing various machine learning algorithms, particularly for tasks like image classification and digit recognition. Its simplicity and accessibility make it an ideal starting point for learning and experimenting with machine learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "373cac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a241647e",
   "metadata": {},
   "source": [
    "### Set the Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ae0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d034a8d",
   "metadata": {},
   "source": [
    "### Download and allocate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e05cf3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numble of train examples:54000\n",
      "The numble of valid examples:6000\n",
      "The numble of test examples:10000\n"
     ]
    }
   ],
   "source": [
    "ROOT = '.data'\n",
    "train_valid_data = torchvision.datasets.MNIST(root=ROOT, train=True, download=True)\n",
    "test_data = torchvision.datasets.MNIST(root=ROOT, train=False, download=False)\n",
    "VALID_RATE = 0.1\n",
    "train_data, valid_data = torch.utils.data.random_split(train_valid_data, \n",
    "                                           [int(len(train_valid_data)*(1-VALID_RATE)),\n",
    "                                            int(len(train_valid_data)* VALID_RATE)])\n",
    "\n",
    "print(f\"The numble of train examples:{len(train_data)}\")\n",
    "print(f\"The numble of valid examples:{len(valid_data)}\")\n",
    "print(f\"The numble of test examples:{len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b95bf7-2ce3-4128-9c3d-8735ebf702be",
   "metadata": {},
   "source": [
    "### Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "527dfb6e-c759-4514-96e9-bc47b422741f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated mean:0.13066047430038452\n",
      "Calculated std:0.30810779333114624\n"
     ]
    }
   ],
   "source": [
    "mean = train_valid_data.data.float().mean()/255.0\n",
    "std = train_valid_data.data.float().std()/255.0\n",
    "\n",
    "print(f\"Calculated mean:{mean}\")\n",
    "print(f\"Calculated std:{std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f869e7e6",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0abb21a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "train_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomRotation(5, fill=(0,)),\n",
    "    torchvision.transforms.RandomCrop(28, padding=2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[mean], std=[std])\n",
    "])\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[mean], std=[std])\n",
    "])\n",
    "\n",
    "train_data.dataset.transform = train_transforms\n",
    "valid_data.dataset.transform = test_transforms\n",
    "test_data.transform = test_transforms\n",
    "\n",
    "print(train_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c4c5a",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "200969b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e27b99",
   "metadata": {},
   "source": [
    "### Load model and Start to train&valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dd0ca0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Now start training -----\n",
      "----- Start training with epoch 1 -----\n",
      "This epoch training time is 13.109920740127563 seconds.\n",
      "Training step: 211, Average Loss: 0.6538264137732475\n",
      "The total Loss on valid dataset: 0.29621782153844833\n",
      "The Accuracy on valid dataset: 91.33%\n",
      "----- Start training with epoch 2 -----\n",
      "This epoch training time is 14.110039710998535 seconds.\n",
      "Training step: 422, Average Loss: 0.2334205402299691\n",
      "The total Loss on valid dataset: 0.21801014679173628\n",
      "The Accuracy on valid dataset: 93.38%\n",
      "----- Start training with epoch 3 -----\n",
      "This epoch training time is 14.256385803222656 seconds.\n",
      "Training step: 633, Average Loss: 0.17274865327994404\n",
      "The total Loss on valid dataset: 0.1724542664984862\n",
      "The Accuracy on valid dataset: 94.75%\n",
      "----- Start training with epoch 4 -----\n",
      "This epoch training time is 13.889003992080688 seconds.\n",
      "Training step: 844, Average Loss: 0.13630749940236597\n",
      "The total Loss on valid dataset: 0.14969749003648758\n",
      "The Accuracy on valid dataset: 95.63%\n",
      "----- Start training with epoch 5 -----\n",
      "This epoch training time is 14.21035099029541 seconds.\n",
      "Training step: 1055, Average Loss: 0.11119958820148101\n",
      "The total Loss on valid dataset: 0.13249193659673134\n",
      "The Accuracy on valid dataset: 95.92%\n",
      "----- Start training with epoch 6 -----\n",
      "This epoch training time is 14.18154764175415 seconds.\n",
      "Training step: 1266, Average Loss: 0.09436889368846518\n",
      "The total Loss on valid dataset: 0.11698471646135052\n",
      "The Accuracy on valid dataset: 96.60%\n",
      "----- Start training with epoch 7 -----\n",
      "This epoch training time is 14.039937734603882 seconds.\n",
      "Training step: 1477, Average Loss: 0.08134439780036985\n",
      "The total Loss on valid dataset: 0.10922235483303666\n",
      "The Accuracy on valid dataset: 96.82%\n",
      "----- Start training with epoch 8 -----\n",
      "This epoch training time is 14.039375305175781 seconds.\n",
      "Training step: 1688, Average Loss: 0.06976739777984778\n",
      "The total Loss on valid dataset: 0.10052160841102402\n",
      "The Accuracy on valid dataset: 96.95%\n",
      "----- Start training with epoch 9 -----\n",
      "This epoch training time is 13.645576000213623 seconds.\n",
      "Training step: 1899, Average Loss: 0.0613796783178621\n",
      "The total Loss on valid dataset: 0.0980698235022525\n",
      "The Accuracy on valid dataset: 97.13%\n",
      "----- Start training with epoch 10 -----\n",
      "This epoch training time is 13.408303499221802 seconds.\n",
      "Training step: 2110, Average Loss: 0.053924532863201126\n",
      "The total Loss on valid dataset: 0.08764589655523498\n",
      "The Accuracy on valid dataset: 97.30%\n",
      "----- Start training with epoch 11 -----\n",
      "This epoch training time is 13.04129147529602 seconds.\n",
      "Training step: 2321, Average Loss: 0.047783393840972\n",
      "The total Loss on valid dataset: 0.08652140960718195\n",
      "The Accuracy on valid dataset: 97.45%\n",
      "----- Start training with epoch 12 -----\n",
      "This epoch training time is 13.137372255325317 seconds.\n",
      "Training step: 2532, Average Loss: 0.04280182657436737\n",
      "The total Loss on valid dataset: 0.08339218407248457\n",
      "The Accuracy on valid dataset: 97.58%\n",
      "----- Start training with epoch 13 -----\n",
      "This epoch training time is 12.867441177368164 seconds.\n",
      "Training step: 2743, Average Loss: 0.037696580548210165\n",
      "The total Loss on valid dataset: 0.08203574859847625\n",
      "The Accuracy on valid dataset: 97.62%\n",
      "----- Start training with epoch 14 -----\n",
      "This epoch training time is 12.900746583938599 seconds.\n",
      "Training step: 2954, Average Loss: 0.03461240676465616\n",
      "The total Loss on valid dataset: 0.07908556361993153\n",
      "The Accuracy on valid dataset: 97.77%\n",
      "----- Start training with epoch 15 -----\n",
      "This epoch training time is 12.902425050735474 seconds.\n",
      "Training step: 3165, Average Loss: 0.030568822273781514\n",
      "The total Loss on valid dataset: 0.07769900451724727\n",
      "The Accuracy on valid dataset: 97.73%\n",
      "----- Start training with epoch 16 -----\n",
      "This epoch training time is 12.978077411651611 seconds.\n",
      "Training step: 3376, Average Loss: 0.027739001718301202\n",
      "The total Loss on valid dataset: 0.08107939766099055\n",
      "The Accuracy on valid dataset: 97.60%\n",
      "----- Start training with epoch 17 -----\n",
      "This epoch training time is 13.189993858337402 seconds.\n",
      "Training step: 3587, Average Loss: 0.025270108265969992\n",
      "The total Loss on valid dataset: 0.07960818257803719\n",
      "The Accuracy on valid dataset: 97.83%\n",
      "----- Start training with epoch 18 -----\n",
      "This epoch training time is 12.957396268844604 seconds.\n",
      "Training step: 3798, Average Loss: 0.022846672984096112\n",
      "The total Loss on valid dataset: 0.07420594346088667\n",
      "The Accuracy on valid dataset: 97.85%\n",
      "----- Start training with epoch 19 -----\n",
      "This epoch training time is 12.829815864562988 seconds.\n",
      "Training step: 4009, Average Loss: 0.021009068326998098\n",
      "The total Loss on valid dataset: 0.07396223326213658\n",
      "The Accuracy on valid dataset: 97.87%\n",
      "----- Start training with epoch 20 -----\n",
      "This epoch training time is 13.048162937164307 seconds.\n",
      "Training step: 4220, Average Loss: 0.019176914621479985\n",
      "The total Loss on valid dataset: 0.07119049690663815\n",
      "The Accuracy on valid dataset: 97.97%\n"
     ]
    }
   ],
   "source": [
    "model = MLP(input_size=28*28, output_size=10)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "if torch.cuda.is_available():\n",
    "    loss_fn = loss_fn.cuda()\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "scaler = GradScaler()\n",
    "\n",
    "total_train_step = 0\n",
    "total_valid_step = 0\n",
    "epoch = 20\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"----- Now start training -----\")\n",
    "for i in range(epoch):\n",
    "    print(f\"----- Start training with epoch {i + 1} -----\")\n",
    "    epoch_train_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for data in train_dataloader:\n",
    "        inputs, targets = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        optimizer.zero_grad()\n",
    "        scaler.update()\n",
    "\n",
    "        total_train_step += 1\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    end_time = time.time()\n",
    "    avg_epoch_train_loss = epoch_train_loss / len(train_dataloader)\n",
    "    print(f\"This epoch training time is {end_time - start_time} seconds.\")\n",
    "    print(f\"Training step: {total_train_step}, Average Loss: {avg_epoch_train_loss}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.eval()\n",
    "    total_valid_loss = 0\n",
    "    total_accuracy = 0\n",
    "    num_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_dataloader:\n",
    "            inputs, targets = data\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_valid_loss += loss.item()\n",
    "            accuracy = (outputs.argmax(1) == targets).sum().item()\n",
    "            total_accuracy += accuracy\n",
    "            num_samples += len(inputs)\n",
    "\n",
    "    avg_valid_loss = total_valid_loss / len(valid_dataloader)\n",
    "    avg_accuracy = total_accuracy / num_samples * 100\n",
    "    print(f\"The total Loss on valid dataset: {avg_valid_loss}\")\n",
    "    print(f\"The Accuracy on valid dataset: {avg_accuracy:.2f}%\")\n",
    "    total_valid_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c898c22",
   "metadata": {},
   "source": [
    "### Go testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4c71a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Test Results -----\n",
      "The total Loss on test dataset: 0.06165867616291507\n",
      "The Accuracy on test dataset: 98.10%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "total_test_loss = 0\n",
    "total_test_accuracy = 0\n",
    "num_samples = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        inputs, targets = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        total_test_loss += loss.item()\n",
    "        accuracy = (outputs.argmax(1) == targets).sum().item()\n",
    "        total_test_accuracy += accuracy\n",
    "        num_samples += len(inputs)\n",
    "\n",
    "avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "avg_test_accuracy = total_test_accuracy / num_samples * 100\n",
    "\n",
    "print(\"----- Test Results -----\")\n",
    "print(f\"The total Loss on test dataset: {avg_test_loss}\")\n",
    "print(f\"The Accuracy on test dataset: {avg_test_accuracy:.2f}%\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdl",
   "language": "python",
   "name": "bdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
