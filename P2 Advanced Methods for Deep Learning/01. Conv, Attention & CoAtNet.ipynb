{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "056ec306-409d-4e84-94aa-429668a995da",
   "metadata": {},
   "source": [
    "# Conv, Attention & CoAtNet\n",
    "\n",
    "In Part 1, we delved into the foundational concepts of deep learning, covering Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). We highlighted that while ViTs can outperform CNNs, they typically require large datasets and extensive computational resources. Now in this chapter let's explore how we can combine the strengths of convolution and attention mechanisms.\n",
    "\n",
    "## Convolution and Attention: Strengths and Weaknesses\n",
    "\n",
    "### Convolutional Neural Networks (CNNs)\n",
    "\n",
    "**Strengths:**\n",
    "- **Locality and Translation Invariance:** CNNs are designed to recognize patterns and features in local receptive fields, making them highly effective at capturing spatial hierarchies in images.\n",
    "- **Parameter Efficiency:** Weight sharing in convolutions significantly reduces the number of parameters compared to fully connected layers.\n",
    "- **Computational Efficiency:** CNNs are highly optimized for parallel computation on modern hardware, making them fast and efficient for image processing tasks.\n",
    "\n",
    "**Weaknesses:**\n",
    "- **Limited Global Context:** Due to their local receptive fields, CNNs may struggle to capture long-range dependencies and global context in images.\n",
    "- **Fixed Architecture:** The predefined nature of convolutional kernels can limit the flexibility and adaptability of CNNs to various data distributions.\n",
    "\n",
    "### Vision Transformers (ViTs)\n",
    "\n",
    "**Strengths:**\n",
    "- **Global Context Capture:** ViTs use self-attention mechanisms to model long-range dependencies and global relationships between different parts of an image.\n",
    "- **Flexibility:** The attention mechanism allows ViTs to adapt to various data structures and distributions without being constrained by local receptive fields.\n",
    "\n",
    "**Weaknesses:**\n",
    "- **Data and Compute Intensive:** ViTs generally require large datasets and substantial computational resources for effective training.\n",
    "- **Parameter Heavy:** The self-attention mechanism can lead to a large number of parameters, making ViTs less efficient in terms of memory usage compared to CNNs.\n",
    "- **Generalization ability:** Although Transformer models have a large model capacity, their generalization ability may not be as good as convolutional networks due to the lack of appropriate induction bias.\n",
    "\n",
    "## Combining Convolution and Attention: The Idea Behind CoAtNet\n",
    "\n",
    "To leverage the strengths of both CNNs and ViTs, researchers have proposed hybrid architectures that integrate convolutional layers and attention mechanisms. One prominent example of such an architecture is CoAtNet (Convolution and Attention Network).\n",
    "\n",
    "### CoAtNet: An Overview\n",
    "\n",
    "CoAtNet is designed to combine the efficiency of CNNs with the global context modeling capability of attention mechanisms. It consists of several stages, each utilizing different combinations of convolution and attention blocks:\n",
    "\n",
    "1. **Initial Convolutional Layers:** The initial stages use convolutional layers to efficiently capture local features and build a strong spatial hierarchy.\n",
    "2. **Transition to Attention:** In the middle stages, CoAtNet gradually introduces attention mechanisms to capture global dependencies and relationships.\n",
    "3. **Attention Dominated Stages:** The final stages rely more on self-attention to refine and enhance the global context understanding.\n",
    "\n",
    "This progressive combination ensures that CoAtNet benefits from the parameter and computational efficiency of convolutions in the early stages, while also leveraging the powerful global feature extraction capabilities of attention mechanisms in the later stages.\n",
    "\n",
    "### CoAtNet Architecture\n",
    "\n",
    "Here is a simplified implementation of CoAtNet in PyTorch to give you an idea of how such a model can be constructed:\n",
    "\n",
    "The same as ViT, the CoAtNet has a deepe structure.It means that If you want to see its performance, you need train it on a big dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d01ae963-25a4-48f5-87dd-c560169fef20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1000])\n",
      "CoAtNet(\n",
      "  (s1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (s2): Sequential(\n",
      "    (0): MBConvBlock(\n",
      "      (expand_conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (dw_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (se): SqueezeExcite(\n",
      "        (se): Sequential(\n",
      "          (0): AdaptiveAvgPool2d(output_size=1)\n",
      "          (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (2): ReLU()\n",
      "          (3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (project_conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): MBConvBlock(\n",
      "      (expand_conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (dw_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (se): SqueezeExcite(\n",
      "        (se): Sequential(\n",
      "          (0): AdaptiveAvgPool2d(output_size=1)\n",
      "          (1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (2): ReLU()\n",
      "          (3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (project_conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (s3): Sequential(\n",
      "    (0): MBConvBlock(\n",
      "      (expand_conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (dw_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (se): SqueezeExcite(\n",
      "        (se): Sequential(\n",
      "          (0): AdaptiveAvgPool2d(output_size=1)\n",
      "          (1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (2): ReLU()\n",
      "          (3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (project_conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): MBConvBlock(\n",
      "      (expand_conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (dw_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (se): SqueezeExcite(\n",
      "        (se): Sequential(\n",
      "          (0): AdaptiveAvgPool2d(output_size=1)\n",
      "          (1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (2): ReLU()\n",
      "          (3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (project_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (s4): Sequential(\n",
      "    (0): MBConvBlock(\n",
      "      (expand_conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (dw_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1024, bias=False)\n",
      "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (se): SqueezeExcite(\n",
      "        (se): Sequential(\n",
      "          (0): AdaptiveAvgPool2d(output_size=1)\n",
      "          (1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (2): ReLU()\n",
      "          (3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (project_conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): MBConvBlock(\n",
      "      (expand_conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn0): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (dw_conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
      "      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (se): SqueezeExcite(\n",
      "        (se): Sequential(\n",
      "          (0): AdaptiveAvgPool2d(output_size=1)\n",
      "          (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (2): ReLU()\n",
      "          (3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (4): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (project_conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (s5): Sequential(\n",
      "    (0): AttentionBlock(\n",
      "      (mhsa): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SqueezeExcite(nn.Module):\n",
    "    def __init__(self, in_ch, reduction=4):\n",
    "        super(SqueezeExcite, self).__init__()\n",
    "        reduced_ch = in_ch // reduction\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_ch, reduced_ch, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(reduced_ch, in_ch, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)\n",
    "\n",
    "class MBConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, expand_ratio=4, reduction=4):\n",
    "        super(MBConvBlock, self).__init__()\n",
    "        mid_ch = in_ch * expand_ratio\n",
    "        self.stride = stride\n",
    "        self.expand = in_ch != out_ch\n",
    "        self.expand_conv = nn.Conv2d(in_ch, mid_ch, 1, bias=False)\n",
    "        self.bn0 = nn.BatchNorm2d(mid_ch)\n",
    "        self.dw_conv = nn.Conv2d(mid_ch, mid_ch, kernel_size, stride, padding=(kernel_size-1)//2, groups=mid_ch, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(mid_ch)\n",
    "        self.se = SqueezeExcite(mid_ch, reduction)\n",
    "        self.project_conv = nn.Conv2d(mid_ch, out_ch, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.expand_conv(x)\n",
    "        x = self.bn0(x)\n",
    "        x = F.relu6(x)\n",
    "        x = self.dw_conv(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu6(x)\n",
    "        x = self.se(x)\n",
    "        x = self.project_conv(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.stride == 1 and self.expand:\n",
    "            x += identity\n",
    "        return x\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, dropout=0.1):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.mhsa = nn.MultiheadAttention(dim, num_heads, dropout=dropout)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.view(b, c, h * w).permute(2, 0, 1)  # (seq_len, batch_size, dim)\n",
    "        x = self.norm(x)\n",
    "        x, _ = self.mhsa(x, x, x)\n",
    "        x = x.permute(1, 2, 0).view(b, c, h, w)  # (batch_size, dim, height, width)\n",
    "        return x\n",
    "\n",
    "class CoAtNet(nn.Module):\n",
    "    def __init__(self, image_size=224, in_channels=3, num_classes=1000):\n",
    "        super(CoAtNet, self).__init__()\n",
    "        self.s1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.s2 = nn.Sequential(\n",
    "            MBConvBlock(64, 128, stride=2),\n",
    "            MBConvBlock(128, 128, stride=1)\n",
    "        )\n",
    "        self.s3 = nn.Sequential(\n",
    "            MBConvBlock(128, 256, stride=2),\n",
    "            MBConvBlock(256, 256, stride=1)\n",
    "        )\n",
    "        self.s4 = nn.Sequential(\n",
    "            MBConvBlock(256, 512, stride=2),\n",
    "            MBConvBlock(512, 512, stride=1)\n",
    "        )\n",
    "        self.s5 = nn.Sequential(\n",
    "            AttentionBlock(512, num_heads=8),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.s1(x)\n",
    "        x = self.s2(x)\n",
    "        x = self.s3(x)\n",
    "        x = self.s4(x)\n",
    "        x = self.s5(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "model = CoAtNet(image_size=224, in_channels=3, num_classes=1000)\n",
    "input_tensor = torch.randn(8, 3, 224, 224)  # Batch of 8, 3x224x224 images\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  # Should output torch.Size([8, 1000])\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdl",
   "language": "python",
   "name": "bdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
