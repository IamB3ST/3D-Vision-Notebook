{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6f8326-b72b-4495-a75a-6fe1d0083b7b",
   "metadata": {},
   "source": [
    "# Attention likes Conv, Swim Transformer\n",
    "\n",
    "In previous chapters, we revisited the strengths and weaknesses of convolutions and attention mechanisms, introduced CoAtNet—a model that combines these two paradigms—and explored ConvNeXt, a convolutional neural network inspired by the Vision Transformer (ViT) architecture. In this chapter, we will delve into Swin Transformer, a novel hierarchical vision transformer that has achieved impressive results across various computer vision tasks.\n",
    "\n",
    "## Introduction to Swin Transformer\n",
    "\n",
    "![Swim Transformer](../imgs/SwimTransformer.jpg)<br>\n",
    "The Swin Transformer, introduced by Liu et al., is a groundbreaking architecture that applies hierarchical design principles to Vision Transformers. Swin stands for Shifted Window, referring to its unique approach to handling image patches. This model effectively captures both local and global features, addressing some of the limitations of previous transformer-based models.\n",
    "\n",
    "## Key Innovations in Swin Transformer\n",
    "\n",
    "### 1. Shifted Window Attention\n",
    "\n",
    "Traditional Vision Transformers use global attention mechanisms, which can be computationally expensive and difficult to scale for high-resolution images. Swin Transformer introduces shifted window attention, where self-attention is computed within local windows, and the windows are shifted between successive layers. This design reduces computational complexity and enhances the model's ability to capture fine-grained details.\n",
    "\n",
    "### 2. Hierarchical Feature Maps\n",
    "\n",
    "Unlike ViTs, which maintain a fixed-size feature map throughout the network, Swin Transformer employs a hierarchical structure similar to convolutional neural networks. The model progressively reduces the spatial dimensions of the feature maps while increasing the number of channels. This design allows the Swin Transformer to handle high-resolution images more efficiently and capture features at multiple scales.\n",
    "\n",
    "### 3. Efficient Computation\n",
    "\n",
    "By computing self-attention within local windows and progressively merging feature maps, Swin Transformer achieves significant computational savings. This efficiency makes it feasible to apply the model to a wide range of vision tasks, from image classification to object detection and segmentation.\n",
    "\n",
    "### 4. Swin Transformer Block\n",
    "\n",
    "The Swin Transformer block is the fundamental building block of the model. Here's a breakdown of a typical Swin Transformer block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8753ecdb-6143-44ef-95a1-de245158ccfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\bdl\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Anaconda\\envs\\bdl\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "D:\\Anaconda\\envs\\bdl\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=3, embed_dim=96, patch_size=4):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, C, H, W) -> (B, H*W, C)\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, drop=0.):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, window_size, num_heads):\n",
    "        super(WindowAttention, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.attn_drop = nn.Dropout(0.0)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4).unbind(0)\n",
    "        \n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0, mlp_ratio=4., drop=0.):\n",
    "        super(SwinTransformerBlock, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WindowAttention(dim, window_size=window_size, num_heads=num_heads)\n",
    "        self.drop_path = nn.Identity()\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=int(dim * mlp_ratio), out_features=dim, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        x_windows = window_partition(shifted_x, self.window_size)\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n",
    "\n",
    "        attn_windows = self.attn(x_windows)\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        x = x.view(B, H * W, C)\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925e7d59-3dbf-4e90-b4f8-0d7b7d53d8f4",
   "metadata": {},
   "source": [
    "### 5. Swin Transformer Macro Architecture\n",
    "The Swin Transformer consists of multiple stages, each containing several Swin Transformer blocks. The number of blocks and their configurations can be adjusted to create different variants of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c6b1d71-4d05-46e8-9d88-20236c3b341d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1000])\n",
      "SwinTransformer(\n",
      "  (patch_embed): PatchEmbedding(\n",
      "    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-5): 6 x SwinTransformerBlock(\n",
      "      (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): WindowAttention(\n",
      "        (qkv): Linear(in_features=96, out_features=288, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): Linear(in_features=96, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000, embed_dim=96, depth=6, num_heads=3, window_size=7, mlp_ratio=4., drop_rate=0.):\n",
    "        super(SwinTransformer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = embed_dim\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(in_channels=in_chans, embed_dim=embed_dim, patch_size=patch_size)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=embed_dim, input_resolution=(img_size // patch_size, img_size // patch_size), num_heads=num_heads, window_size=window_size, shift_size=0 if i % 2 == 0 else window_size // 2, mlp_ratio=mlp_ratio, drop=drop_rate)\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "# Test the simplified Swin Transformer with a sample input\n",
    "input_tensor = torch.randn(8, 3, 224, 224)  # Batch of 8, 3x224x224 images\n",
    "model = SwinTransformer()\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  # Expected output shape: (8, 1000)\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdl",
   "language": "python",
   "name": "bdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
