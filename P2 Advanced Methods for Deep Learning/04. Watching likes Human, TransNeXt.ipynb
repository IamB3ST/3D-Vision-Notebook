{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c00372-7a8f-48ba-b0f9-69a20196dfb0",
   "metadata": {},
   "source": [
    "# TransNeXt: Robust Foveal Visual Perception for Vision Transformers\n",
    "\n",
    "In previous chapters, we revisited the strengths and weaknesses of convolutions and attention mechanisms, introduced CoAtNet—a model that combines these two paradigms—and explored ConvNeXt, a convolutional neural network inspired by the Vision Transformer (ViT) architecture. We also delved into Swin Transformer, which integrates a sliding window approach into the Vision Transformer framework. In this chapter, we will explore TransNeXt, a novel approach that enhances Vision Transformers with robust foveal visual perception.\n",
    "\n",
    "## Introduction to TransNeXt\n",
    "\n",
    "![TransNeXt](../imgs/TransNeXt.jpg)\n",
    "Vision Transformers (ViTs) have revolutionized the field of computer vision by leveraging self-attention mechanisms to capture global context. However, ViTs often struggle with robustness and fine-grained details, which are critical for tasks requiring high precision. TransNeXt addresses these challenges by incorporating foveal visual perception principles, enhancing the robustness and detail sensitivity of ViTs.\n",
    "\n",
    "## Advantages of TransNeXt\n",
    "\n",
    "### Robustness\n",
    "\n",
    "By emulating the human visual system, TransNeXt enhances the robustness of Vision Transformers. The model becomes more resilient to variations and distortions in the input data, improving performance on tasks with challenging conditions.\n",
    "\n",
    "### Precision\n",
    "\n",
    "The high-resolution foveal attention mechanism allows TransNeXt to focus on fine details, improving accuracy in tasks requiring detailed visual analysis. This makes TransNeXt particularly effective for applications such as medical imaging and fine-grained object recognition.\n",
    "\n",
    "### Efficiency\n",
    "\n",
    "TransNeXt achieves a balance between computational efficiency and performance. By dynamically adjusting the attention span, the model allocates resources more effectively, maintaining high performance without excessive computational overhead.\n",
    "\n",
    "## Key Innovations in TransNeXt\n",
    "\n",
    "### 1. High-Resolution Foveal Attention\n",
    "\n",
    "TransNeXt introduces a high-resolution foveal attention mechanism, focusing on a smaller, more detailed region of the image. This is achieved by dynamically adjusting the attention span, allowing the model to allocate more computational resources to the regions requiring finer detail.\n",
    "\n",
    "### 2. Low-Resolution Peripheral Attention\n",
    "\n",
    "In parallel, TransNeXt employs a low-resolution peripheral attention mechanism, capturing the broader context of the image. This dual attention mechanism ensures that the model maintains a balance between detail and context, improving both precision and robustness.\n",
    "\n",
    "### 3. Hierarchical Feature Maps\n",
    "\n",
    "Similar to Swin Transformer, TransNeXt uses a hierarchical structure. The model progressively reduces the spatial dimensions of the feature maps while increasing the number of channels. This design allows TransNeXt to handle high-resolution images more efficiently and capture features at multiple scales.\n",
    "\n",
    "### 4. Efficient Computation\n",
    "\n",
    "By computing self-attention within local windows and progressively merging feature maps, TransNeXt achieves significant computational savings. This efficiency makes it feasible to apply the model to a wide range of vision tasks, from image classification to object detection and segmentation.\n",
    "\n",
    "### 5. TransNeXt Block\n",
    "\n",
    "The TransNeXt block is the fundamental building block of the model. Here's a breakdown of a typical TransNeXt block:\n",
    "- **High-Resolution Attention**: Focuses on small, detailed regions.\n",
    "- **Low-Resolution Attention**: Captures broader, contextual information.\n",
    "- **Merging Mechanism**: Integrates the outputs from high and low-resolution attention mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5b81296-cb74-4d60-9bde-62d513cbe8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "\n",
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super(DWConv, self).__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, bias=True, groups=dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        x = x.transpose(1, 2).view(B, C, H, W).contiguous()\n",
    "        x = self.dwconv(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class ConvolutionalGLU(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super(ConvolutionalGLU, self).__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        hidden_features = int(2 * hidden_features / 3)\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features * 2)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        x, v = self.fc1(x).chunk(2, dim=-1)\n",
    "        x = self.act(self.dwconv(x, H, W)) * v\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_relative_position_cpb(query_size, key_size, pretrain_size=None):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    pretrain_size = pretrain_size or query_size\n",
    "    axis_qh = torch.arange(query_size[0], dtype=torch.float32, device=device)\n",
    "    axis_kh = F.adaptive_avg_pool1d(axis_qh.unsqueeze(0), key_size[0]).squeeze(0)\n",
    "    axis_qw = torch.arange(query_size[1], dtype=torch.float32, device=device)\n",
    "    axis_kw = F.adaptive_avg_pool1d(axis_qw.unsqueeze(0), key_size[1]).squeeze(0)\n",
    "    axis_kh, axis_kw = torch.meshgrid(axis_kh, axis_kw)\n",
    "    axis_qh, axis_qw = torch.meshgrid(axis_qh, axis_qw)\n",
    "    axis_kh = torch.reshape(axis_kh, [-1])\n",
    "    axis_kw = torch.reshape(axis_kw, [-1])\n",
    "    axis_qh = torch.reshape(axis_qh, [-1])\n",
    "    axis_qw = torch.reshape(axis_qw, [-1])\n",
    "    relative_h = (axis_qh[:, None] - axis_kh[None, :]) / (pretrain_size[0] - 1) * 8\n",
    "    relative_w = (axis_qw[:, None] - axis_kw[None, :]) / (pretrain_size[1] - 1) * 8\n",
    "    relative_hw = torch.stack([relative_h, relative_w], dim=-1).view(-1, 2)\n",
    "    relative_coords_table, idx_map = torch.unique(relative_hw, return_inverse=True, dim=0)\n",
    "    relative_coords_table = torch.sign(relative_coords_table) * torch.log2(\n",
    "        torch.abs(relative_coords_table) + 1.0) / torch.log2(torch.tensor(8, dtype=torch.float32))\n",
    "    return idx_map, relative_coords_table\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, num_heads=8, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.temperature = nn.Parameter(torch.log((torch.ones(num_heads, 1, 1) / 0.24).exp() - 1))\n",
    "        self.register_buffer(\"seq_length_scale\", torch.as_tensor(np.log(input_resolution[0] * input_resolution[1])), persistent=False)\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.query_embedding = nn.Parameter(nn.init.trunc_normal_(torch.empty(self.num_heads, 1, self.head_dim), mean=0, std=0.02))\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.cpb_fc1 = nn.Linear(2, 512, bias=True)\n",
    "        self.cpb_act = nn.ReLU(inplace=True)\n",
    "        self.cpb_fc2 = nn.Linear(512, num_heads, bias=True)\n",
    "\n",
    "    def forward(self, x, H, W, relative_pos_index, relative_coords_table):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, -1, 3 * self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=1)\n",
    "        rel_bias = self.cpb_fc2(self.cpb_act(self.cpb_fc1(relative_coords_table))).transpose(0, 1)[:, relative_pos_index.view(-1)].view(-1, N, N)\n",
    "        attn = ((F.normalize(q, dim=-1) + self.query_embedding) * F.softplus(self.temperature) * self.seq_length_scale) @ F.normalize(k, dim=-1).transpose(-2, -1) + rel_bias\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, input_resolution, window_size=3, mlp_ratio=4.,\n",
    "                 qkv_bias=False, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1, fixed_pool_size=None):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, input_resolution, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = ConvolutionalGLU(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x, H, W, relative_pos_index, relative_coords_table):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x), H, W, relative_pos_index, relative_coords_table))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489fdc3e-65a0-4896-b785-ded884b20b6b",
   "metadata": {},
   "source": [
    "### 6. TransNeXt Macro Architecture\n",
    "\n",
    "The TransNeXt consists of multiple stages, each containing several TransNeXt blocks. The number of blocks and their configurations can be adjusted to create different variants of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f17c81-792a-42b2-8e5b-10f149bc6c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverlapPatchEmbed(nn.Module):\n",
    "    def __init__(self, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride, padding=(patch_size[0] // 2, patch_size[1] // 2))\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        _, _, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x, H, W\n",
    "\n",
    "class TransNeXt(nn.Module):\n",
    "    def __init__(self, img_size=224, pretrain_size=None, window_size=[3, 3, 3, None], patch_size=16, in_chans=3, num_classes=1000, embed_dims=[64, 128, 256, 512], num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm, depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1], num_stages=4, fixed_pool_size=None):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.depths = depths\n",
    "        self.num_stages = num_stages\n",
    "        pretrain_size = pretrain_size or img_size\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        cur = 0\n",
    "        for i in range(num_stages):\n",
    "            patch_embed = OverlapPatchEmbed(patch_size=7 if i == 0 else 3, stride=4 if i == 0 else 2, in_chans=in_chans if i == 0 else embed_dims[i - 1], embed_dim=embed_dims[i])\n",
    "            num_patches = (pretrain_size // (4 if i == 0 else 2)) ** 2\n",
    "            relative_pos_index, relative_coords_table = get_relative_position_cpb((pretrain_size // (4 if i == 0 else 2), pretrain_size // (4 if i == 0 else 2)), (pretrain_size // (4 if i == 0 else 2), pretrain_size // (4 if i == 0 else 2)))\n",
    "            blocks = nn.ModuleList([Block(dim=embed_dims[i], num_heads=num_heads[i], input_resolution=(pretrain_size // (4 if i == 0 else 2), pretrain_size // (4 if i == 0 else 2)), window_size=window_size[i], mlp_ratio=mlp_ratios[i], qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + j], norm_layer=norm_layer, sr_ratio=sr_ratios[i], fixed_pool_size=fixed_pool_size) for j in range(depths[i])])\n",
    "            cur += depths[i]\n",
    "            norm = norm_layer(embed_dims[i])\n",
    "            setattr(self, f\"patch_embed{i + 1}\", patch_embed)\n",
    "            setattr(self, f\"blocks{i + 1}\", blocks)\n",
    "            setattr(self, f\"norm{i + 1}\", norm)\n",
    "        self.head = nn.Linear(embed_dims[-1], num_classes) if num_classes > 0 else nn.Identity()\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_stages):\n",
    "            patch_embed = getattr(self, f\"patch_embed{i + 1}\")\n",
    "            blocks = getattr(self, f\"blocks{i + 1}\")\n",
    "            norm = getattr(self, f\"norm{i + 1}\")\n",
    "            x, H, W = patch_embed(x)\n",
    "            for blk in blocks:\n",
    "                x = blk(x, H, W)\n",
    "            x = norm(x)\n",
    "            if i < self.num_stages - 1:\n",
    "                x = x.permute(0, 2, 1).view(x.size(0), -1, H, W)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "input_tensor = torch.randn(8, 3, 224, 224)  # Batch of 8, 3x224x224 images\n",
    "model = TransNeXt()\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  # Expected output shape: (8, 1000)\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdl",
   "language": "python",
   "name": "bdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
