{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c00372-7a8f-48ba-b0f9-69a20196dfb0",
   "metadata": {},
   "source": [
    "# Watching likes Human, TransNeXt\n",
    "\n",
    "In previous chapters, we revisited the strengths and weaknesses of convolutions and attention mechanisms, introduced CoAtNet—a model that combines these two paradigms—and explored ConvNeXt, a convolutional neural network inspired by the Vision Transformer (ViT) architecture. We also delved into Swin Transformer, which integrates a sliding window approach into the Vision Transformer framework. In this chapter, we will explore TransNeXt, a novel approach that enhances Vision Transformers with robust foveal visual perception.\n",
    "\n",
    "## Introduction to TransNeXt\n",
    "\n",
    "![TransNeXt](../imgs/TransNeXt.jpg)\n",
    "Vision Transformers (ViTs) have revolutionized the field of computer vision by leveraging self-attention mechanisms to capture global context. However, ViTs often struggle with robustness and fine-grained details, which are critical for tasks requiring high precision. TransNeXt addresses these challenges by incorporating foveal visual perception principles, enhancing the robustness and detail sensitivity of ViTs.\n",
    "\n",
    "## Advantages of TransNeXt\n",
    "\n",
    "### Robustness\n",
    "\n",
    "By emulating the human visual system, TransNeXt enhances the robustness of Vision Transformers. The model becomes more resilient to variations and distortions in the input data, improving performance on tasks with challenging conditions.\n",
    "\n",
    "### Precision\n",
    "\n",
    "The high-resolution foveal attention mechanism allows TransNeXt to focus on fine details, improving accuracy in tasks requiring detailed visual analysis. This makes TransNeXt particularly effective for applications such as medical imaging and fine-grained object recognition.\n",
    "\n",
    "### Efficiency\n",
    "\n",
    "TransNeXt achieves a balance between computational efficiency and performance. By dynamically adjusting the attention span, the model allocates resources more effectively, maintaining high performance without excessive computational overhead.\n",
    "\n",
    "## Key Innovations in TransNeXt\n",
    "\n",
    "### 1. High-Resolution Foveal Attention\n",
    "\n",
    "TransNeXt introduces a high-resolution foveal attention mechanism, focusing on a smaller, more detailed region of the image. This is achieved by dynamically adjusting the attention span, allowing the model to allocate more computational resources to the regions requiring finer detail.\n",
    "\n",
    "### 2. Low-Resolution Peripheral Attention\n",
    "\n",
    "In parallel, TransNeXt employs a low-resolution peripheral attention mechanism, capturing the broader context of the image. This dual attention mechanism ensures that the model maintains a balance between detail and context, improving both precision and robustness.\n",
    "\n",
    "### 3. Hierarchical Feature Maps\n",
    "\n",
    "Similar to Swin Transformer, TransNeXt uses a hierarchical structure. The model progressively reduces the spatial dimensions of the feature maps while increasing the number of channels. This design allows TransNeXt to handle high-resolution images more efficiently and capture features at multiple scales.\n",
    "\n",
    "### 4. Efficient Computation\n",
    "\n",
    "By computing self-attention within local windows and progressively merging feature maps, TransNeXt achieves significant computational savings. This efficiency makes it feasible to apply the model to a wide range of vision tasks, from image classification to object detection and segmentation.\n",
    "\n",
    "### 5. TransNeXt Block\n",
    "\n",
    "The TransNeXt block is the fundamental building block of the model. Here's a breakdown of a typical TransNeXt block:\n",
    "- **High-Resolution Attention**: Focuses on small, detailed regions.\n",
    "- **Low-Resolution Attention**: Captures broader, contextual information.\n",
    "- **Merging Mechanism**: Integrates the outputs from high and low-resolution attention mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5b81296-cb74-4d60-9bde-62d513cbe8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\bdl\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Anaconda\\envs\\bdl\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "D:\\Anaconda\\envs\\bdl\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "import math\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_seqlen_and_mask(input_resolution, window_size):\n",
    "    attn_map = F.unfold(torch.ones([1, 1, input_resolution[0], input_resolution[1]]), window_size,\n",
    "                        dilation=1, padding=(window_size // 2, window_size // 2), stride=1)\n",
    "    attn_local_length = attn_map.sum(-2).squeeze().unsqueeze(-1)\n",
    "    attn_mask = (attn_map.squeeze(0).permute(1, 0)) == 0\n",
    "    return attn_local_length, attn_mask\n",
    "\n",
    "\n",
    "class AggregatedAttention(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, num_heads=8, window_size=3, qkv_bias=True,\n",
    "                 attn_drop=0., proj_drop=0., sr_ratio=1, fixed_pool_size=None):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n",
    "\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "\n",
    "        self.sr_ratio = sr_ratio\n",
    "\n",
    "        assert window_size % 2 == 1, \"window size must be odd\"\n",
    "        self.window_size = window_size\n",
    "        self.local_len = window_size ** 2\n",
    "\n",
    "        if fixed_pool_size is None:\n",
    "            self.pool_H, self.pool_W = input_resolution[0] // self.sr_ratio, input_resolution[1] // self.sr_ratio\n",
    "        else:\n",
    "            assert fixed_pool_size < min(input_resolution), \\\n",
    "                f\"The fixed_pool_size {fixed_pool_size} should be less than the shorter side of input resolution {input_resolution} to ensure pooling works correctly.\"\n",
    "            self.pool_H, self.pool_W = fixed_pool_size, fixed_pool_size\n",
    "        self.pool_len = self.pool_H * self.pool_W\n",
    "\n",
    "        self.unfold = nn.Unfold(kernel_size=window_size, padding=window_size // 2, stride=1)\n",
    "        self.temperature = nn.Parameter(\n",
    "            torch.log((torch.ones(num_heads, 1, 1) / 0.24).exp() - 1))  # Initialize softplus(temperature) to 1/0.24.\n",
    "\n",
    "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.query_embedding = nn.Parameter(\n",
    "            nn.init.trunc_normal_(torch.empty(self.num_heads, 1, self.head_dim), mean=0, std=0.02))\n",
    "        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        # Components to generate pooled features.\n",
    "        self.pool = nn.AdaptiveAvgPool2d((self.pool_H, self.pool_W))\n",
    "        self.sr = nn.Conv2d(dim, dim, kernel_size=1, stride=1, padding=0)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "        # mlp to generate continuous relative position bias\n",
    "        self.cpb_fc1 = nn.Linear(2, 512, bias=True)\n",
    "        self.cpb_act = nn.ReLU(inplace=True)\n",
    "        self.cpb_fc2 = nn.Linear(512, num_heads, bias=True)\n",
    "\n",
    "        # relative bias for local features\n",
    "        self.relative_pos_bias_local = nn.Parameter(\n",
    "            nn.init.trunc_normal_(torch.empty(num_heads, self.local_len), mean=0, std=0.0004))\n",
    "\n",
    "        # Generate padding_mask && sequnce length scale\n",
    "        local_seq_length, padding_mask = get_seqlen_and_mask(input_resolution, window_size)\n",
    "        self.register_buffer(\"seq_length_scale\", torch.as_tensor(np.log(local_seq_length.numpy() + self.pool_len)),\n",
    "                             persistent=False)\n",
    "        self.register_buffer(\"padding_mask\", padding_mask, persistent=False)\n",
    "\n",
    "        # dynamic_local_bias:\n",
    "        self.learnable_tokens = nn.Parameter(\n",
    "            nn.init.trunc_normal_(torch.empty(num_heads, self.head_dim, self.local_len), mean=0, std=0.02))\n",
    "        self.learnable_bias = nn.Parameter(torch.zeros(num_heads, 1, self.local_len))\n",
    "\n",
    "    def forward(self, x, H, W, relative_pos_index, relative_coords_table):\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # Generate queries, normalize them with L2, add query embedding, and then magnify with sequence length scale and temperature.\n",
    "        # Use softplus function ensuring that the temperature is not lower than 0.\n",
    "        q_norm = F.normalize(self.q(x).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3), dim=-1)\n",
    "        q_norm_scaled = (q_norm + self.query_embedding) * F.softplus(self.temperature) * self.seq_length_scale\n",
    "\n",
    "        # Generate unfolded keys and values and l2-normalize them\n",
    "        k_local, v_local = self.kv(x).chunk(2, dim=-1)\n",
    "        k_local = F.normalize(k_local.reshape(B, N, self.num_heads, self.head_dim), dim=-1).reshape(B, N, -1)\n",
    "        kv_local = torch.cat([k_local, v_local], dim=-1).permute(0, 2, 1).reshape(B, -1, H, W)\n",
    "        k_local, v_local = self.unfold(kv_local).reshape(\n",
    "            B, 2 * self.num_heads, self.head_dim, self.local_len, N).permute(0, 1, 4, 2, 3).chunk(2, dim=1)\n",
    "\n",
    "        # Compute local similarity\n",
    "        attn_local = ((q_norm_scaled.unsqueeze(-2) @ k_local).squeeze(-2) \\\n",
    "                      + self.relative_pos_bias_local.unsqueeze(1)).masked_fill(self.padding_mask, float('-inf'))\n",
    "\n",
    "        # Generate pooled features\n",
    "        x_ = x.permute(0, 2, 1).reshape(B, -1, H, W).contiguous()\n",
    "        x_ = self.pool(self.act(self.sr(x_))).reshape(B, -1, self.pool_len).permute(0, 2, 1)\n",
    "        x_ = self.norm(x_)\n",
    "\n",
    "        # Generate pooled keys and values\n",
    "        kv_pool = self.kv(x_).reshape(B, self.pool_len, 2 * self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k_pool, v_pool = kv_pool.chunk(2, dim=1)\n",
    "\n",
    "        # Use MLP to generate continuous relative positional bias for pooled features.\n",
    "        pool_bias = self.cpb_fc2(self.cpb_act(self.cpb_fc1(relative_coords_table))).transpose(0, 1)[:,\n",
    "                    relative_pos_index.view(-1)].view(-1, N, self.pool_len)\n",
    "        # Compute pooled similarity\n",
    "        attn_pool = q_norm_scaled @ F.normalize(k_pool, dim=-1).transpose(-2, -1) + pool_bias\n",
    "\n",
    "        # Concatenate local & pooled similarity matrices and calculate attention weights through the same Softmax\n",
    "        attn = torch.cat([attn_local, attn_pool], dim=-1).softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # Split the attention weights and separately aggregate the values of local & pooled features\n",
    "        attn_local, attn_pool = torch.split(attn, [self.local_len, self.pool_len], dim=-1)\n",
    "        x_local = (((q_norm @ self.learnable_tokens) + self.learnable_bias + attn_local).unsqueeze(\n",
    "            -2) @ v_local.transpose(-2, -1)).squeeze(-2)\n",
    "        x_pool = attn_pool @ v_pool\n",
    "        x = (x_local + x_pool).transpose(1, 2).reshape(B, N, C)\n",
    "\n",
    "        # Linear projection and output\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super(DWConv, self).__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, bias=True, groups=dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        x = x.transpose(1, 2).view(B, C, H, W).contiguous()\n",
    "        x = self.dwconv(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvolutionalGLU(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        hidden_features = int(2 * hidden_features / 3)\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features * 2)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        x, v = self.fc1(x).chunk(2, dim=-1)\n",
    "        x = self.act(self.dwconv(x, H, W)) * v\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_relative_position_cpb(query_size, key_size, pretrain_size=None):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    pretrain_size = pretrain_size or query_size\n",
    "    axis_qh = torch.arange(query_size[0], dtype=torch.float32, device=device)\n",
    "    axis_kh = F.adaptive_avg_pool1d(axis_qh.unsqueeze(0), key_size[0]).squeeze(0)\n",
    "    axis_qw = torch.arange(query_size[1], dtype=torch.float32, device=device)\n",
    "    axis_kw = F.adaptive_avg_pool1d(axis_qw.unsqueeze(0), key_size[1]).squeeze(0)\n",
    "    axis_kh, axis_kw = torch.meshgrid(axis_kh, axis_kw)\n",
    "    axis_qh, axis_qw = torch.meshgrid(axis_qh, axis_qw)\n",
    "\n",
    "    axis_kh = torch.reshape(axis_kh, [-1])\n",
    "    axis_kw = torch.reshape(axis_kw, [-1])\n",
    "    axis_qh = torch.reshape(axis_qh, [-1])\n",
    "    axis_qw = torch.reshape(axis_qw, [-1])\n",
    "\n",
    "    relative_h = (axis_qh[:, None] - axis_kh[None, :]) / (pretrain_size[0] - 1) * 8\n",
    "    relative_w = (axis_qw[:, None] - axis_kw[None, :]) / (pretrain_size[1] - 1) * 8\n",
    "    relative_hw = torch.stack([relative_h, relative_w], dim=-1).view(-1, 2)\n",
    "\n",
    "    relative_coords_table, idx_map = torch.unique(relative_hw, return_inverse=True, dim=0)\n",
    "\n",
    "    relative_coords_table = torch.sign(relative_coords_table) * torch.log2(\n",
    "        torch.abs(relative_coords_table) + 1.0) / torch.log2(torch.tensor(8, dtype=torch.float32))\n",
    "\n",
    "    return idx_map, relative_coords_table\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, num_heads=8, qkv_bias=True, attn_drop=0.,\n",
    "                 proj_drop=0.):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n",
    "\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.temperature = nn.Parameter(\n",
    "            torch.log((torch.ones(num_heads, 1, 1) / 0.24).exp() - 1))  # Initialize softplus(temperature) to 1/0.24.\n",
    "        # Generate sequnce length scale\n",
    "        self.register_buffer(\"seq_length_scale\", torch.as_tensor(np.log(input_resolution[0] * input_resolution[1])),\n",
    "                             persistent=False)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.query_embedding = nn.Parameter(\n",
    "            nn.init.trunc_normal_(torch.empty(self.num_heads, 1, self.head_dim), mean=0, std=0.02))\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        # mlp to generate continuous relative position bias\n",
    "        self.cpb_fc1 = nn.Linear(2, 512, bias=True)\n",
    "        self.cpb_act = nn.ReLU(inplace=True)\n",
    "        self.cpb_fc2 = nn.Linear(512, num_heads, bias=True)\n",
    "\n",
    "    def forward(self, x, H, W, relative_pos_index, relative_coords_table):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, -1, 3 * self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=1)\n",
    "\n",
    "        # Use MLP to generate continuous relative positional bias\n",
    "        rel_bias = self.cpb_fc2(self.cpb_act(self.cpb_fc1(relative_coords_table))).transpose(0, 1)[:,\n",
    "                   relative_pos_index.view(-1)].view(-1, N, N)\n",
    "\n",
    "        # Calculate attention map using sequence length scaled cosine attention and query embedding\n",
    "        attn = ((F.normalize(q, dim=-1) + self.query_embedding) * F.softplus(\n",
    "            self.temperature) * self.seq_length_scale) @ F.normalize(k, dim=-1).transpose(-2, -1) + rel_bias\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, input_resolution, window_size=3, mlp_ratio=4.,\n",
    "                 qkv_bias=False, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1, fixed_pool_size=None):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        if sr_ratio == 1:\n",
    "            self.attn = Attention(\n",
    "                dim,\n",
    "                input_resolution,\n",
    "                num_heads=num_heads,\n",
    "                qkv_bias=qkv_bias,\n",
    "                attn_drop=attn_drop,\n",
    "                proj_drop=drop)\n",
    "        else:\n",
    "            self.attn = AggregatedAttention(\n",
    "                dim,\n",
    "                input_resolution,\n",
    "                window_size=window_size,\n",
    "                num_heads=num_heads,\n",
    "                qkv_bias=qkv_bias,\n",
    "                attn_drop=attn_drop,\n",
    "                proj_drop=drop,\n",
    "                sr_ratio=sr_ratio,\n",
    "                fixed_pool_size=fixed_pool_size)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = ConvolutionalGLU(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x, H, W, relative_pos_index, relative_coords_table):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x), H, W, relative_pos_index, relative_coords_table))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489fdc3e-65a0-4896-b785-ded884b20b6b",
   "metadata": {},
   "source": [
    "### 6. TransNeXt Macro Architecture\n",
    "\n",
    "The TransNeXt consists of multiple stages, each containing several TransNeXt blocks. The number of blocks and their configurations can be adjusted to create different variants of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93f17c81-792a-42b2-8e5b-10f149bc6c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1000])\n",
      "TransNeXt(\n",
      "  (patch_embed1): OverlapPatchEmbed(\n",
      "    (proj): Conv2d(3, 72, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
      "    (norm): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (block1): ModuleList(\n",
      "    (0-1): 2 x Block(\n",
      "      (norm1): LayerNorm((72,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): AggregatedAttention(\n",
      "        (unfold): Unfold(kernel_size=3, dilation=1, padding=1, stride=1)\n",
      "        (q): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (kv): Linear(in_features=72, out_features=144, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "        (sr): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (norm): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (cpb_fc1): Linear(in_features=2, out_features=512, bias=True)\n",
      "        (cpb_act): ReLU(inplace=True)\n",
      "        (cpb_fc2): Linear(in_features=512, out_features=3, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((72,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): ConvolutionalGLU(\n",
      "        (fc1): Linear(in_features=72, out_features=768, bias=True)\n",
      "        (dwconv): DWConv(\n",
      "          (dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=384, out_features=72, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm((72,), eps=1e-06, elementwise_affine=True)\n",
      "  (patch_embed2): OverlapPatchEmbed(\n",
      "    (proj): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (block2): ModuleList(\n",
      "    (0-1): 2 x Block(\n",
      "      (norm1): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): AggregatedAttention(\n",
      "        (unfold): Unfold(kernel_size=3, dilation=1, padding=1, stride=1)\n",
      "        (q): Linear(in_features=144, out_features=144, bias=True)\n",
      "        (kv): Linear(in_features=144, out_features=288, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=144, out_features=144, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "        (sr): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (cpb_fc1): Linear(in_features=2, out_features=512, bias=True)\n",
      "        (cpb_act): ReLU(inplace=True)\n",
      "        (cpb_fc2): Linear(in_features=512, out_features=6, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): ConvolutionalGLU(\n",
      "        (fc1): Linear(in_features=144, out_features=1536, bias=True)\n",
      "        (dwconv): DWConv(\n",
      "          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=768, out_features=144, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm2): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n",
      "  (patch_embed3): OverlapPatchEmbed(\n",
      "    (proj): Conv2d(144, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (block3): ModuleList(\n",
      "    (0-14): 15 x Block(\n",
      "      (norm1): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): AggregatedAttention(\n",
      "        (unfold): Unfold(kernel_size=3, dilation=1, padding=1, stride=1)\n",
      "        (q): Linear(in_features=288, out_features=288, bias=True)\n",
      "        (kv): Linear(in_features=288, out_features=576, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=288, out_features=288, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "        (sr): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (cpb_fc1): Linear(in_features=2, out_features=512, bias=True)\n",
      "        (cpb_act): ReLU(inplace=True)\n",
      "        (cpb_fc2): Linear(in_features=512, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): ConvolutionalGLU(\n",
      "        (fc1): Linear(in_features=288, out_features=1536, bias=True)\n",
      "        (dwconv): DWConv(\n",
      "          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=768, out_features=288, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm3): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n",
      "  (patch_embed4): OverlapPatchEmbed(\n",
      "    (proj): Conv2d(288, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (norm): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (block4): ModuleList(\n",
      "    (0-1): 2 x Block(\n",
      "      (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=576, out_features=1728, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=576, out_features=576, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (cpb_fc1): Linear(in_features=2, out_features=512, bias=True)\n",
      "        (cpb_act): ReLU(inplace=True)\n",
      "        (cpb_fc2): Linear(in_features=512, out_features=24, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): ConvolutionalGLU(\n",
      "        (fc1): Linear(in_features=576, out_features=3072, bias=True)\n",
      "        (dwconv): DWConv(\n",
      "          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=1536, out_features=576, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm4): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
      "  (head): Linear(in_features=576, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class OverlapPatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "\n",
    "        assert max(patch_size) > stride, \"Set larger patch_size than stride\"\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n",
    "                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        _, _, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, H, W\n",
    "\n",
    "\n",
    "class TransNeXt(nn.Module):\n",
    "    '''\n",
    "    The parameter \"img size\" is primarily utilized for generating relative spatial coordinates,\n",
    "    which are used to compute continuous relative positional biases. As this TransNeXt implementation does not support multi-scale inputs,\n",
    "    it is recommended to set the \"img size\" parameter to a value that is exactly the same as the resolution of the inference images.\n",
    "    It is not advisable to set the \"img size\" parameter to a value exceeding 800x800.\n",
    "    The \"pretrain size\" refers to the \"img size\" used during the initial pre-training phase,\n",
    "    which is used to scale the relative spatial coordinates for better extrapolation by the MLP.\n",
    "    For models trained on ImageNet-1K at a resolution of 224x224,\n",
    "    as well as downstream task models fine-tuned based on these pre-trained weights,\n",
    "    the \"pretrain size\" parameter should be set to 224x224.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, img_size=224, pretrain_size=None, window_size=[3, 3, 3, None],\n",
    "                 patch_size=16, in_chans=3, num_classes=1000, embed_dims=[64, 128, 256, 512],\n",
    "                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, drop_rate=0.,\n",
    "                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n",
    "                 depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1], num_stages=4, fixed_pool_size=None):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.depths = depths\n",
    "        self.num_stages = num_stages\n",
    "        pretrain_size = pretrain_size or img_size\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "        cur = 0\n",
    "\n",
    "        for i in range(num_stages):\n",
    "            # Generate relative positional coordinate table and index for each stage to compute continuous relative positional bias.\n",
    "            relative_pos_index, relative_coords_table = get_relative_position_cpb(\n",
    "                query_size=to_2tuple(img_size // (2 ** (i + 2))),\n",
    "                key_size=to_2tuple(img_size // ((2 ** (i + 2)) * sr_ratios[i])) if (\n",
    "                        fixed_pool_size is None or sr_ratios[i] == 1) else to_2tuple(fixed_pool_size),\n",
    "                pretrain_size=to_2tuple(pretrain_size // (2 ** (i + 2))))\n",
    "\n",
    "            self.register_buffer(f\"relative_pos_index{i + 1}\", relative_pos_index, persistent=False)\n",
    "            self.register_buffer(f\"relative_coords_table{i + 1}\", relative_coords_table, persistent=False)\n",
    "\n",
    "            patch_embed = OverlapPatchEmbed(patch_size=patch_size * 2 - 1 if i == 0 else 3,\n",
    "                                            stride=patch_size if i == 0 else 2,\n",
    "                                            in_chans=in_chans if i == 0 else embed_dims[i - 1],\n",
    "                                            embed_dim=embed_dims[i])\n",
    "\n",
    "            block = nn.ModuleList([Block(\n",
    "                dim=embed_dims[i], input_resolution=to_2tuple(img_size // (2 ** (i + 2))), window_size=window_size[i],\n",
    "                num_heads=num_heads[i], mlp_ratio=mlp_ratios[i], qkv_bias=qkv_bias,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + j], norm_layer=norm_layer,\n",
    "                sr_ratio=sr_ratios[i], fixed_pool_size=fixed_pool_size)\n",
    "                for j in range(depths[i])])\n",
    "            norm = norm_layer(embed_dims[i])\n",
    "            cur += depths[i]\n",
    "\n",
    "            setattr(self, f\"patch_embed{i + 1}\", patch_embed)\n",
    "            setattr(self, f\"block{i + 1}\", block)\n",
    "            setattr(self, f\"norm{i + 1}\", norm)\n",
    "\n",
    "        # classification head\n",
    "        self.head = nn.Linear(embed_dims[3], num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        for n, m in self.named_modules():\n",
    "            self._init_weights(m, n)\n",
    "\n",
    "    def _init_weights(self, m: nn.Module, name: str = ''):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm2d)):\n",
    "            nn.init.zeros_(m.bias)\n",
    "            nn.init.ones_(m.weight)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'query_embedding', 'relative_pos_bias_local', 'cpb', 'temperature'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "\n",
    "        for i in range(self.num_stages):\n",
    "            patch_embed = getattr(self, f\"patch_embed{i + 1}\")\n",
    "            block = getattr(self, f\"block{i + 1}\")\n",
    "            norm = getattr(self, f\"norm{i + 1}\")\n",
    "            x, H, W = patch_embed(x)\n",
    "            relative_pos_index = getattr(self, f\"relative_pos_index{i + 1}\")\n",
    "            relative_coords_table = getattr(self, f\"relative_coords_table{i + 1}\")\n",
    "            for blk in block:\n",
    "                x = blk(x, H, W, relative_pos_index, relative_coords_table)\n",
    "            x = norm(x)\n",
    "            if i != self.num_stages - 1:\n",
    "                x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        return x.mean(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "input_tensor = torch.randn(8, 3, 224, 224)  # Batch of 8, 3x224x224 images\n",
    "model = TransNeXt(window_size=[3, 3, 3, None],\n",
    "                      patch_size=4, embed_dims=[72, 144, 288, 576], num_heads=[3, 6, 12, 24],\n",
    "                      mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n",
    "                      norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 15, 2], sr_ratios=[8, 4, 2, 1],\n",
    "                      )\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  # Expected output shape: (8, 1000)\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdl",
   "language": "python",
   "name": "bdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
