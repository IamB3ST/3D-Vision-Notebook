{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc4b81d6-8f39-4dc3-96dd-0f701c14e6bf",
   "metadata": {},
   "source": [
    "# Gated structure & MogaNet\n",
    "\n",
    "In previous chapters, we revisited the strengths and weaknesses of convolutions and attention mechanisms, introduced CoAtNet—a model that combines these two paradigms—and explored ConvNeXt, a convolutional neural network inspired by the Vision Transformer (ViT) architecture. We also delved into Swin Transformer, which integrates a sliding window approach into the Vision Transformer framework, and introduced TransNeXt, a model that enhances ViTs with robust foveal visual perception. In this chapter, we will explore gated structures in deep learning, their evolution from GRU to GLU, and some of the latest models utilizing these mechanisms. Following that, we will introduce MogaNet, its core concepts, and architecture.\n",
    "\n",
    "## Introduction to Gated Structures in Deep Learning\n",
    "\n",
    "Gated structures have become a fundamental component in deep learning, particularly in the realm of sequence modeling and neural network architectures. Gated mechanisms control the flow of information through the network, enabling it to selectively retain and forget information, which is crucial for capturing long-term dependencies and improving model efficiency.\n",
    "\n",
    "### History of Gated Structures\n",
    "\n",
    "#### 1. Gated Recurrent Unit (GRU)\n",
    "\n",
    "The Gated Recurrent Unit (GRU), introduced by Cho et al. in 2014, was designed to address the vanishing gradient problem in recurrent neural networks (RNNs). GRUs use gating mechanisms to control the flow of information and maintain long-term dependencies. The key components of a GRU are:\n",
    "- **Reset Gate**: Determines how much of the past information to forget.\n",
    "- **Update Gate**: Controls how much of the past information to retain and how much of the new information to incorporate.\n",
    "\n",
    "#### 2. Gated Linear Unit (GLU)\n",
    "\n",
    "The Gated Linear Unit (GLU), proposed by Dauphin et al. in 2017, extends the concept of gating to feedforward neural networks. GLUs use a gating mechanism similar to GRUs but are applied to linear transformations rather than recurrent connections. The key idea is to use element-wise multiplication of two linear transformations, allowing the model to learn which features to pass through and which to suppress.\n",
    "\n",
    "### Recent Advances in Gated Structures\n",
    "\n",
    "Recent models have continued to leverage and refine gated mechanisms to enhance performance and efficiency. Some notable examples include:\n",
    "- **Gated Convolutional Neural Networks (GCNNs)**: Introduce gating mechanisms to convolutional layers, allowing for dynamic control over spatial features.\n",
    "- **Gated Self-Attention**: Combines self-attention mechanisms with gating to selectively focus on relevant parts of the input sequence.\n",
    "- **GLU Variants**: Various adaptations of GLUs, such as the Sigmoid Linear Unit (SiLU) and Swish activation functions, have been proposed to improve the gating mechanism.\n",
    "\n",
    "### Why Gated Structures are Effective\n",
    "\n",
    "Gated structures are effective because they:\n",
    "- **Control Information Flow**: Enable selective retention and suppression of information, improving learning efficiency.\n",
    "- **Capture Long-Term Dependencies**: Address issues like vanishing gradients, making it easier to capture long-term dependencies in sequential data.\n",
    "- **Enhance Model Expressiveness**: Allow models to learn more complex patterns and relationships in the data.\n",
    "\n",
    "## Introduction to MogaNet\n",
    "\n",
    "MogaNet represents a novel approach that incorporates the principles of gated structures into a unified architecture designed for robust and efficient deep learning. The core innovations of MogaNet include multi-scale spatial gated aggregation, a channel aggregation module (CA Block), and a hierarchical architecture.\n",
    "\n",
    "### Core Concepts of MogaNet\n",
    "\n",
    "#### 1. Multi-Scale Spatial Gated Aggregation\n",
    "\n",
    "![MogaNet](../imgs/MogaNet.jpg)<br>\n",
    "One of MogaNet's key innovations is its multi-scale spatial gated aggregation mechanism, which allows the network to capture both local details and global context information simultaneously. By using depthwise convolutions (DWConv) with different dilation rates, the network can focus on features at various scales, thus encoding both low-level (local) and high-level (global) features.\n",
    "\n",
    "#### 2. Channel Aggregation Module (CA Block)\n",
    "\n",
    "![MogaNetBlock](../imgs/MogaNetBlock.jpg)<br>\n",
    "Another innovative aspect of MogaNet is the CA Block, which reduces redundancy in feature representations and enhances the network's ability to recognize important features. The CA Block reallocates channel information through a simple projection and activation function, effectively integrating inter-channel information and improving feature expressiveness.\n",
    "\n",
    "#### 3. Hierarchical Architecture\n",
    "\n",
    "MogaNet employs a hierarchical design, meaning the network progressively extracts and refines features through multiple processing stages. Each stage consists of several Moga Blocks that handle features at different resolutions.\n",
    "\n",
    "### MogaNet Architecture\n",
    "\n",
    "The MogaNet architecture comprises several key components:\n",
    "\n",
    "#### 1. Embedding Stem\n",
    "\n",
    "The network input first passes through the embedding stem, which converts the input image into a set of feature maps while reducing its spatial dimensions. The embedding stem typically consists of several convolutional layers to extract initial features.\n",
    "\n",
    "#### 2. Moga Blocks\n",
    "\n",
    "Each Moga Block consists of two main parts: spatial aggregation and channel aggregation.\n",
    "\n",
    "- **Spatial Aggregation**: Captures features at different scales using multi-scale depthwise convolution layers with varying dilation rates. This enables the network to capture features ranging from local to global contexts.\n",
    "- **Gated Aggregation**: Uses the SiLU activation function to control the fusion of different features, helping the network focus on the most useful information.\n",
    "\n",
    "#### 3. Channel Aggregation Module (CA Block)\n",
    "\n",
    "After the Moga Blocks, features pass through the CA Block, which mixes features using a depthwise convolution layer followed by a 1x1 convolution layer. The CA Block allows the network to adaptively adjust the weights between channels, optimizing the feature representation.\n",
    "\n",
    "#### 4. Multi-Stage Processing\n",
    "\n",
    "MogaNet is composed of multiple such stages, each progressively reducing the spatial resolution of the features while increasing their depth, capturing increasingly complex features.\n",
    "\n",
    "#### 5. Global Average Pooling (GAP) and Classifier\n",
    "\n",
    "After all stages, the feature map is aggregated using Global Average Pooling (GAP) to generate a global feature vector. This vector is then fed into a linear layer or classifier to perform the final task, such as image classification.\n",
    "\n",
    "### MogaNet Block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a9e5dc6-247c-4430-a290-c469972a4e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import trunc_normal_, DropPath\n",
    "\n",
    "\n",
    "def build_act_layer(act_type):\n",
    "    \"\"\"Build activation layer.\"\"\"\n",
    "    if act_type is None:\n",
    "        return nn.Identity()\n",
    "    assert act_type in ['GELU', 'ReLU', 'SiLU']\n",
    "    if act_type == 'SiLU':\n",
    "        return nn.SiLU()\n",
    "    elif act_type == 'ReLU':\n",
    "        return nn.ReLU()\n",
    "    else:\n",
    "        return nn.GELU()\n",
    "\n",
    "\n",
    "def build_norm_layer(norm_type, embed_dims):\n",
    "    \"\"\"Build normalization layer.\"\"\"\n",
    "    assert norm_type in ['BN', 'GN', 'LN2d', 'SyncBN']\n",
    "    if norm_type == 'GN':\n",
    "        return nn.GroupNorm(embed_dims, embed_dims, eps=1e-5)\n",
    "    if norm_type == 'LN2d':\n",
    "        return LayerNorm2d(embed_dims, eps=1e-6)\n",
    "    if norm_type == 'SyncBN':\n",
    "        return nn.SyncBatchNorm(embed_dims, eps=1e-5)\n",
    "    else:\n",
    "        return nn.BatchNorm2d(embed_dims, eps=1e-5)\n",
    "\n",
    "\n",
    "class LayerNorm2d(nn.Module):\n",
    "    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs \n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 normalized_shape,\n",
    "                 eps=1e-6,\n",
    "                 data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        assert self.data_format in [\"channels_last\", \"channels_first\"] \n",
    "        self.normalized_shape = (normalized_shape, )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "            return x\n",
    "\n",
    "\n",
    "class ElementScale(nn.Module):\n",
    "    \"\"\"A learnable element-wise scaler.\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dims, init_value=0., requires_grad=True):\n",
    "        super(ElementScale, self).__init__()\n",
    "        self.scale = nn.Parameter(\n",
    "            init_value * torch.ones((1, embed_dims, 1, 1)),\n",
    "            requires_grad=requires_grad\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.scale\n",
    "\n",
    "\n",
    "class ChannelAggregationFFN(nn.Module):\n",
    "    \"\"\"An implementation of FFN with Channel Aggregation.\n",
    "\n",
    "    Args:\n",
    "        embed_dims (int): The feature dimension. Same as\n",
    "            `MultiheadAttention`.\n",
    "        feedforward_channels (int): The hidden dimension of FFNs.\n",
    "        kernel_size (int): The depth-wise conv kernel size as the\n",
    "            depth-wise convolution. Defaults to 3.\n",
    "        act_type (str): The type of activation. Defaults to 'GELU'.\n",
    "        ffn_drop (float, optional): Probability of an element to be\n",
    "            zeroed in FFN. Default 0.0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embed_dims,\n",
    "                 feedforward_channels,\n",
    "                 kernel_size=3,\n",
    "                 act_type='GELU',\n",
    "                 ffn_drop=0.):\n",
    "        super(ChannelAggregationFFN, self).__init__()\n",
    "\n",
    "        self.embed_dims = embed_dims\n",
    "        self.feedforward_channels = feedforward_channels\n",
    "\n",
    "        self.fc1 = nn.Conv2d(\n",
    "            in_channels=embed_dims,\n",
    "            out_channels=self.feedforward_channels,\n",
    "            kernel_size=1)\n",
    "        self.dwconv = nn.Conv2d(\n",
    "            in_channels=self.feedforward_channels,\n",
    "            out_channels=self.feedforward_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=kernel_size // 2,\n",
    "            bias=True,\n",
    "            groups=self.feedforward_channels)\n",
    "        self.act = build_act_layer(act_type)\n",
    "        self.fc2 = nn.Conv2d(\n",
    "            in_channels=feedforward_channels,\n",
    "            out_channels=embed_dims,\n",
    "            kernel_size=1)\n",
    "        self.drop = nn.Dropout(ffn_drop)\n",
    "\n",
    "        self.decompose = nn.Conv2d(\n",
    "            in_channels=self.feedforward_channels,  # C -> 1\n",
    "            out_channels=1, kernel_size=1,\n",
    "        )\n",
    "        self.sigma = ElementScale(\n",
    "            self.feedforward_channels, init_value=1e-5, requires_grad=True)\n",
    "        self.decompose_act = build_act_layer(act_type)\n",
    "\n",
    "    def feat_decompose(self, x):\n",
    "        # x_d: [B, C, H, W] -> [B, 1, H, W]\n",
    "        x = x + self.sigma(x - self.decompose_act(self.decompose(x)))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # proj 1\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        # proj 2\n",
    "        x = self.feat_decompose(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiOrderDWConv(nn.Module):\n",
    "    \"\"\"Multi-order Features with Dilated DWConv Kernel.\n",
    "\n",
    "    Args:\n",
    "        embed_dims (int): Number of input channels.\n",
    "        dw_dilation (list): Dilations of three DWConv layers.\n",
    "        channel_split (list): The raletive ratio of three splited channels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embed_dims,\n",
    "                 dw_dilation=[1, 2, 3,],\n",
    "                 channel_split=[1, 3, 4,],\n",
    "                ):\n",
    "        super(MultiOrderDWConv, self).__init__()\n",
    "\n",
    "        self.split_ratio = [i / sum(channel_split) for i in channel_split]\n",
    "        self.embed_dims_1 = int(self.split_ratio[1] * embed_dims)\n",
    "        self.embed_dims_2 = int(self.split_ratio[2] * embed_dims)\n",
    "        self.embed_dims_0 = embed_dims - self.embed_dims_1 - self.embed_dims_2\n",
    "        self.embed_dims = embed_dims\n",
    "        assert len(dw_dilation) == len(channel_split) == 3\n",
    "        assert 1 <= min(dw_dilation) and max(dw_dilation) <= 3\n",
    "        assert embed_dims % sum(channel_split) == 0\n",
    "\n",
    "        # basic DW conv\n",
    "        self.DW_conv0 = nn.Conv2d(\n",
    "            in_channels=self.embed_dims,\n",
    "            out_channels=self.embed_dims,\n",
    "            kernel_size=5,\n",
    "            padding=(1 + 4 * dw_dilation[0]) // 2,\n",
    "            groups=self.embed_dims,\n",
    "            stride=1, dilation=dw_dilation[0],\n",
    "        )\n",
    "        # DW conv 1\n",
    "        self.DW_conv1 = nn.Conv2d(\n",
    "            in_channels=self.embed_dims_1,\n",
    "            out_channels=self.embed_dims_1,\n",
    "            kernel_size=5,\n",
    "            padding=(1 + 4 * dw_dilation[1]) // 2,\n",
    "            groups=self.embed_dims_1,\n",
    "            stride=1, dilation=dw_dilation[1],\n",
    "        )\n",
    "        # DW conv 2\n",
    "        self.DW_conv2 = nn.Conv2d(\n",
    "            in_channels=self.embed_dims_2,\n",
    "            out_channels=self.embed_dims_2,\n",
    "            kernel_size=7,\n",
    "            padding=(1 + 6 * dw_dilation[2]) // 2,\n",
    "            groups=self.embed_dims_2,\n",
    "            stride=1, dilation=dw_dilation[2],\n",
    "        )\n",
    "        # a channel convolution\n",
    "        self.PW_conv = nn.Conv2d(  # point-wise convolution\n",
    "            in_channels=embed_dims,\n",
    "            out_channels=embed_dims,\n",
    "            kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_0 = self.DW_conv0(x)\n",
    "        x_1 = self.DW_conv1(\n",
    "            x_0[:, self.embed_dims_0: self.embed_dims_0+self.embed_dims_1, ...])\n",
    "        x_2 = self.DW_conv2(\n",
    "            x_0[:, self.embed_dims-self.embed_dims_2:, ...])\n",
    "        x = torch.cat([\n",
    "            x_0[:, :self.embed_dims_0, ...], x_1, x_2], dim=1)\n",
    "        x = self.PW_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiOrderGatedAggregation(nn.Module):\n",
    "    \"\"\"Spatial Block with Multi-order Gated Aggregation.\n",
    "\n",
    "    Args:\n",
    "        embed_dims (int): Number of input channels.\n",
    "        attn_dw_dilation (list): Dilations of three DWConv layers.\n",
    "        attn_channel_split (list): The raletive ratio of splited channels.\n",
    "        attn_act_type (str): The activation type for Spatial Block.\n",
    "            Defaults to 'SiLU'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embed_dims,\n",
    "                 attn_dw_dilation=[1, 2, 3],\n",
    "                 attn_channel_split=[1, 3, 4],\n",
    "                 attn_act_type='SiLU',\n",
    "                 attn_force_fp32=False,\n",
    "                ):\n",
    "        super(MultiOrderGatedAggregation, self).__init__()\n",
    "\n",
    "        self.embed_dims = embed_dims\n",
    "        self.attn_force_fp32 = attn_force_fp32\n",
    "        self.proj_1 = nn.Conv2d(\n",
    "            in_channels=embed_dims, out_channels=embed_dims, kernel_size=1)\n",
    "        self.gate = nn.Conv2d(\n",
    "            in_channels=embed_dims, out_channels=embed_dims, kernel_size=1)\n",
    "        self.value = MultiOrderDWConv(\n",
    "            embed_dims=embed_dims,\n",
    "            dw_dilation=attn_dw_dilation,\n",
    "            channel_split=attn_channel_split,\n",
    "        )\n",
    "        self.proj_2 = nn.Conv2d(\n",
    "            in_channels=embed_dims, out_channels=embed_dims, kernel_size=1)\n",
    "\n",
    "        # activation for gating and value\n",
    "        self.act_value = build_act_layer(attn_act_type)\n",
    "        self.act_gate = build_act_layer(attn_act_type)\n",
    "\n",
    "        # decompose\n",
    "        self.sigma = ElementScale(\n",
    "            embed_dims, init_value=1e-5, requires_grad=True)\n",
    "\n",
    "    def feat_decompose(self, x):\n",
    "        x = self.proj_1(x)\n",
    "        # x_d: [B, C, H, W] -> [B, C, 1, 1]\n",
    "        x_d = F.adaptive_avg_pool2d(x, output_size=1)\n",
    "        x = x + self.sigma(x - x_d)\n",
    "        x = self.act_value(x)\n",
    "        return x\n",
    "\n",
    "    def forward_gating(self, g, v):\n",
    "        with torch.autocast(device_type='cuda', enabled=False):\n",
    "            g = g.to(torch.float32)\n",
    "            v = v.to(torch.float32)\n",
    "            return self.proj_2(self.act_gate(g) * self.act_gate(v))\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x.clone()\n",
    "        # proj 1x1\n",
    "        x = self.feat_decompose(x)\n",
    "        # gating and value branch\n",
    "        g = self.gate(x)\n",
    "        v = self.value(x)\n",
    "        # aggregation\n",
    "        if not self.attn_force_fp32:\n",
    "            x = self.proj_2(self.act_gate(g) * self.act_gate(v))\n",
    "        else:\n",
    "            x = self.forward_gating(self.act_gate(g), self.act_gate(v))\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "\n",
    "\n",
    "class MogaBlock(nn.Module):\n",
    "    \"\"\"A block of MogaNet.\n",
    "\n",
    "    Args:\n",
    "        embed_dims (int): Number of input channels.\n",
    "        ffn_ratio (float): The expansion ratio of feedforward network hidden\n",
    "            layer channels. Defaults to 4.\n",
    "        drop_rate (float): Dropout rate after embedding. Defaults to 0.\n",
    "        drop_path_rate (float): Stochastic depth rate. Defaults to 0.1.\n",
    "        act_type (str): The activation type for projections and FFNs.\n",
    "            Defaults to 'GELU'.\n",
    "        norm_cfg (str): The type of normalization layer. Defaults to 'BN'.\n",
    "        init_value (float): Init value for Layer Scale. Defaults to 1e-5.\n",
    "        attn_dw_dilation (list): Dilations of three DWConv layers.\n",
    "        attn_channel_split (list): The raletive ratio of splited channels.\n",
    "        attn_act_type (str): The activation type for the gating branch.\n",
    "            Defaults to 'SiLU'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embed_dims,\n",
    "                 ffn_ratio=4.,\n",
    "                 drop_rate=0.,\n",
    "                 drop_path_rate=0.,\n",
    "                 act_type='GELU',\n",
    "                 norm_type='BN',\n",
    "                 init_value=1e-5,\n",
    "                 attn_dw_dilation=[1, 2, 3],\n",
    "                 attn_channel_split=[1, 3, 4],\n",
    "                 attn_act_type='SiLU',\n",
    "                 attn_force_fp32=False,\n",
    "                ):\n",
    "        super(MogaBlock, self).__init__()\n",
    "        self.out_channels = embed_dims\n",
    "\n",
    "        self.norm1 = build_norm_layer(norm_type, embed_dims)\n",
    "\n",
    "        # spatial attention\n",
    "        self.attn = MultiOrderGatedAggregation(\n",
    "            embed_dims,\n",
    "            attn_dw_dilation=attn_dw_dilation,\n",
    "            attn_channel_split=attn_channel_split,\n",
    "            attn_act_type=attn_act_type,\n",
    "            attn_force_fp32=attn_force_fp32,\n",
    "        )\n",
    "        self.drop_path = DropPath(\n",
    "            drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = build_norm_layer(norm_type, embed_dims)\n",
    "\n",
    "        # channel MLP\n",
    "        mlp_hidden_dim = int(embed_dims * ffn_ratio)\n",
    "        self.mlp = ChannelAggregationFFN(  # DWConv + Channel Aggregation FFN\n",
    "            embed_dims=embed_dims,\n",
    "            feedforward_channels=mlp_hidden_dim,\n",
    "            act_type=act_type,\n",
    "            ffn_drop=drop_rate,\n",
    "        )\n",
    "\n",
    "        # init layer scale\n",
    "        self.layer_scale_1 = nn.Parameter(\n",
    "            init_value * torch.ones((1, embed_dims, 1, 1)), requires_grad=True)\n",
    "        self.layer_scale_2 = nn.Parameter(\n",
    "            init_value * torch.ones((1, embed_dims, 1, 1)), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # spatial\n",
    "        identity = x\n",
    "        x = self.layer_scale_1 * self.attn(self.norm1(x))\n",
    "        x = identity + self.drop_path(x)\n",
    "        # channel\n",
    "        identity = x\n",
    "        x = self.layer_scale_2 * self.mlp(self.norm2(x))\n",
    "        x = identity + self.drop_path(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvPatchEmbed(nn.Module):\n",
    "    \"\"\"An implementation of Conv patch embedding layer.\n",
    "\n",
    "    Args:\n",
    "        in_features (int): The feature dimension.\n",
    "        embed_dims (int): The output dimension of PatchEmbed.\n",
    "        kernel_size (int): The conv kernel size of PatchEmbed.\n",
    "            Defaults to 3.\n",
    "        stride (int): The conv stride of PatchEmbed. Defaults to 2.\n",
    "        norm_type (str): The type of normalization layer. Defaults to 'BN'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 embed_dims,\n",
    "                 kernel_size=3,\n",
    "                 stride=2,\n",
    "                 norm_type='BN'):\n",
    "        super(ConvPatchEmbed, self).__init__()\n",
    "\n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels, embed_dims, kernel_size=kernel_size,\n",
    "            stride=stride, padding=kernel_size // 2)\n",
    "        self.norm = build_norm_layer(norm_type, embed_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = self.norm(x)\n",
    "        out_size = (x.shape[2], x.shape[3])\n",
    "        return x, out_size\n",
    "\n",
    "\n",
    "class StackConvPatchEmbed(nn.Module):\n",
    "    \"\"\"An implementation of Stack Conv patch embedding layer.\n",
    "\n",
    "    Args:\n",
    "        in_features (int): The feature dimension.\n",
    "        embed_dims (int): The output dimension of PatchEmbed.\n",
    "        kernel_size (int): The conv kernel size of stack patch embedding.\n",
    "            Defaults to 3.\n",
    "        stride (int): The conv stride of stack patch embedding.\n",
    "            Defaults to 2.\n",
    "        act_type (str): The activation in PatchEmbed. Defaults to 'GELU'.\n",
    "        norm_type (str): The type of normalization layer. Defaults to 'BN'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 embed_dims,\n",
    "                 kernel_size=3,\n",
    "                 stride=2,\n",
    "                 act_type='GELU',\n",
    "                 norm_type='BN'):\n",
    "        super(StackConvPatchEmbed, self).__init__()\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, embed_dims // 2, kernel_size=kernel_size,\n",
    "                stride=stride, padding=kernel_size // 2),\n",
    "            build_norm_layer(norm_type, embed_dims // 2),\n",
    "            build_act_layer(act_type),\n",
    "            nn.Conv2d(embed_dims // 2, embed_dims, kernel_size=kernel_size,\n",
    "                stride=stride, padding=kernel_size // 2),\n",
    "            build_norm_layer(norm_type, embed_dims),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        out_size = (x.shape[2], x.shape[3])\n",
    "        return x, out_size\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581edc8f-7921-4ebf-8dd6-1500134e5bf5",
   "metadata": {},
   "source": [
    "### MogaNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e44d78e-2bb0-4198-80ef-e4f8f821fc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1000])\n",
      "MogaNet(\n",
      "  (patch_embed1): StackConvPatchEmbed(\n",
      "    (projection): Sequential(\n",
      "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (blocks1): ModuleList(\n",
      "    (0-2): 3 x MogaBlock(\n",
      "      (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (attn): MultiOrderGatedAggregation(\n",
      "        (proj_1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (gate): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (value): MultiOrderDWConv(\n",
      "          (DW_conv0): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)\n",
      "          (DW_conv1): Conv2d(12, 12, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=12)\n",
      "          (DW_conv2): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=16)\n",
      "          (PW_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (proj_2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (act_value): SiLU()\n",
      "        (act_gate): SiLU()\n",
      "        (sigma): ElementScale()\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (mlp): ChannelAggregationFFN(\n",
      "        (fc1): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "        (decompose): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (sigma): ElementScale()\n",
      "        (decompose_act): GELU(approximate='none')\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (patch_embed2): ConvPatchEmbed(\n",
      "    (projection): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (blocks2): ModuleList(\n",
      "    (0-2): 3 x MogaBlock(\n",
      "      (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (attn): MultiOrderGatedAggregation(\n",
      "        (proj_1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (gate): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (value): MultiOrderDWConv(\n",
      "          (DW_conv0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64)\n",
      "          (DW_conv1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=24)\n",
      "          (DW_conv2): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=32)\n",
      "          (PW_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (proj_2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (act_value): SiLU()\n",
      "        (act_gate): SiLU()\n",
      "        (sigma): ElementScale()\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (mlp): ChannelAggregationFFN(\n",
      "        (fc1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "        (decompose): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (sigma): ElementScale()\n",
      "        (decompose_act): GELU(approximate='none')\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (patch_embed3): ConvPatchEmbed(\n",
      "    (projection): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (blocks3): ModuleList(\n",
      "    (0-11): 12 x MogaBlock(\n",
      "      (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (attn): MultiOrderGatedAggregation(\n",
      "        (proj_1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (gate): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (value): MultiOrderDWConv(\n",
      "          (DW_conv0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=128)\n",
      "          (DW_conv1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=48)\n",
      "          (DW_conv2): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=64)\n",
      "          (PW_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (proj_2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (act_value): SiLU()\n",
      "        (act_gate): SiLU()\n",
      "        (sigma): ElementScale()\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (mlp): ChannelAggregationFFN(\n",
      "        (fc1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "        (decompose): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (sigma): ElementScale()\n",
      "        (decompose_act): GELU(approximate='none')\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (patch_embed4): ConvPatchEmbed(\n",
      "    (projection): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (blocks4): ModuleList(\n",
      "    (0-1): 2 x MogaBlock(\n",
      "      (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (attn): MultiOrderGatedAggregation(\n",
      "        (proj_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (gate): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (value): MultiOrderDWConv(\n",
      "          (DW_conv0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)\n",
      "          (DW_conv1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=96)\n",
      "          (DW_conv2): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=128)\n",
      "          (PW_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (proj_2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (act_value): SiLU()\n",
      "        (act_gate): SiLU()\n",
      "        (sigma): ElementScale()\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (mlp): ChannelAggregationFFN(\n",
      "        (fc1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "        (decompose): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (sigma): ElementScale()\n",
      "        (decompose_act): GELU(approximate='none')\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): Linear(in_features=256, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ConvPatchEmbed(nn.Module):\n",
    "    \"\"\"An implementation of Conv patch embedding layer.\n",
    "\n",
    "    Args:\n",
    "        in_features (int): The feature dimension.\n",
    "        embed_dims (int): The output dimension of PatchEmbed.\n",
    "        kernel_size (int): The conv kernel size of PatchEmbed.\n",
    "            Defaults to 3.\n",
    "        stride (int): The conv stride of PatchEmbed. Defaults to 2.\n",
    "        norm_type (str): The type of normalization layer. Defaults to 'BN'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 embed_dims,\n",
    "                 kernel_size=3,\n",
    "                 stride=2,\n",
    "                 norm_type='BN'):\n",
    "        super(ConvPatchEmbed, self).__init__()\n",
    "\n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels, embed_dims, kernel_size=kernel_size,\n",
    "            stride=stride, padding=kernel_size // 2)\n",
    "        self.norm = build_norm_layer(norm_type, embed_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = self.norm(x)\n",
    "        out_size = (x.shape[2], x.shape[3])\n",
    "        return x, out_size\n",
    "\n",
    "\n",
    "class StackConvPatchEmbed(nn.Module):\n",
    "    \"\"\"An implementation of Stack Conv patch embedding layer.\n",
    "\n",
    "    Args:\n",
    "        in_features (int): The feature dimension.\n",
    "        embed_dims (int): The output dimension of PatchEmbed.\n",
    "        kernel_size (int): The conv kernel size of stack patch embedding.\n",
    "            Defaults to 3.\n",
    "        stride (int): The conv stride of stack patch embedding.\n",
    "            Defaults to 2.\n",
    "        act_type (str): The activation in PatchEmbed. Defaults to 'GELU'.\n",
    "        norm_type (str): The type of normalization layer. Defaults to 'BN'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 embed_dims,\n",
    "                 kernel_size=3,\n",
    "                 stride=2,\n",
    "                 act_type='GELU',\n",
    "                 norm_type='BN'):\n",
    "        super(StackConvPatchEmbed, self).__init__()\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, embed_dims // 2, kernel_size=kernel_size,\n",
    "                stride=stride, padding=kernel_size // 2),\n",
    "            build_norm_layer(norm_type, embed_dims // 2),\n",
    "            build_act_layer(act_type),\n",
    "            nn.Conv2d(embed_dims // 2, embed_dims, kernel_size=kernel_size,\n",
    "                stride=stride, padding=kernel_size // 2),\n",
    "            build_norm_layer(norm_type, embed_dims),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        out_size = (x.shape[2], x.shape[3])\n",
    "        return x, out_size\n",
    "\n",
    "\n",
    "class MogaNet(nn.Module):\n",
    "    \n",
    "    arch_zoo = {\n",
    "        **dict.fromkeys(['xt', 'x-tiny', 'xtiny'],\n",
    "                        {'embed_dims': [32, 64, 96, 192],\n",
    "                         'depths': [3, 3, 10, 2],\n",
    "                         'ffn_ratios': [8, 8, 4, 4]}),\n",
    "        **dict.fromkeys(['t', 'tiny'],\n",
    "                        {'embed_dims': [32, 64, 128, 256],\n",
    "                         'depths': [3, 3, 12, 2],\n",
    "                         'ffn_ratios': [8, 8, 4, 4]}),\n",
    "        **dict.fromkeys(['s', 'small'],\n",
    "                        {'embed_dims': [64, 128, 320, 512],\n",
    "                         'depths': [2, 3, 12, 2],\n",
    "                         'ffn_ratios': [8, 8, 4, 4]}),\n",
    "        **dict.fromkeys(['b', 'base'],\n",
    "                        {'embed_dims': [64, 160, 320, 512],\n",
    "                         'depths': [4, 6, 22, 3],\n",
    "                         'ffn_ratios': [8, 8, 4, 4]}),\n",
    "        **dict.fromkeys(['l', 'large'],\n",
    "                        {'embed_dims': [64, 160, 320, 640],\n",
    "                         'depths': [4, 6, 44, 4],\n",
    "                         'ffn_ratios': [8, 8, 4, 4]}),\n",
    "        **dict.fromkeys(['xl', 'x-large', 'xlarge'],\n",
    "                        {'embed_dims': [96, 192, 480, 960],\n",
    "                         'depths': [6, 6, 44, 4],\n",
    "                         'ffn_ratios': [8, 8, 4, 4]}),\n",
    "    } \n",
    "\n",
    "    def __init__(self,\n",
    "                 arch='tiny',\n",
    "                 in_channels=3,\n",
    "                 num_classes=1000,\n",
    "                 drop_rate=0.,\n",
    "                 drop_path_rate=0.,\n",
    "                 init_value=1e-5,\n",
    "                 head_init_scale=1.,\n",
    "                 patch_sizes=[3, 3, 3, 3],\n",
    "                 stem_norm_type='BN',\n",
    "                 conv_norm_type='BN',\n",
    "                 patchembed_types=['ConvEmbed', 'Conv', 'Conv', 'Conv',],\n",
    "                 attn_dw_dilation=[1, 2, 3],\n",
    "                 attn_channel_split=[1, 3, 4],\n",
    "                 attn_act_type='SiLU',\n",
    "                 attn_final_dilation=True,\n",
    "                 attn_force_fp32=False,\n",
    "                 fork_feat=False,\n",
    "                 frozen_stages=-1,\n",
    "                 init_cfg=None,\n",
    "                 pretrained=None,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(arch, str):\n",
    "            arch = arch.lower()\n",
    "            assert arch in set(self.arch_zoo), \\\n",
    "                f'Arch {arch} is not in default archs {set(self.arch_zoo)}'\n",
    "            self.arch_settings = self.arch_zoo[arch]\n",
    "        else:\n",
    "            essential_keys = {'embed_dims', 'depths', 'ffn_ratios'}\n",
    "            assert isinstance(arch, dict) and set(arch) == essential_keys, \\\n",
    "                f'Custom arch needs a dict with keys {essential_keys}'\n",
    "            self.arch_settings = arch\n",
    "\n",
    "        self.embed_dims = self.arch_settings['embed_dims']\n",
    "        self.depths = self.arch_settings['depths']\n",
    "        self.ffn_ratios = self.arch_settings['ffn_ratios']\n",
    "        self.num_stages = len(self.depths)\n",
    "        self.attn_force_fp32 = attn_force_fp32\n",
    "        self.use_layer_norm = stem_norm_type == 'LN'\n",
    "        assert len(patchembed_types) == self.num_stages\n",
    "        self.fork_feat = fork_feat\n",
    "        self.frozen_stages = frozen_stages\n",
    "\n",
    "        total_depth = sum(self.depths)\n",
    "        dpr = [\n",
    "            x.item() for x in torch.linspace(0, drop_path_rate, total_depth)\n",
    "        ]  # stochastic depth decay rule\n",
    "\n",
    "        cur_block_idx = 0\n",
    "        for i, depth in enumerate(self.depths):\n",
    "            if i == 0 and patchembed_types[i] == \"ConvEmbed\":\n",
    "                assert patch_sizes[i] <= 3\n",
    "                patch_embed = StackConvPatchEmbed(\n",
    "                    in_channels=in_channels,\n",
    "                    embed_dims=self.embed_dims[i],\n",
    "                    kernel_size=patch_sizes[i],\n",
    "                    stride=patch_sizes[i] // 2 + 1,\n",
    "                    act_type='GELU',\n",
    "                    norm_type=conv_norm_type,\n",
    "                )\n",
    "            else:\n",
    "                patch_embed = ConvPatchEmbed(\n",
    "                    in_channels=in_channels if i == 0 else self.embed_dims[i - 1],\n",
    "                    embed_dims=self.embed_dims[i],\n",
    "                    kernel_size=patch_sizes[i],\n",
    "                    stride=patch_sizes[i] // 2 + 1,\n",
    "                    norm_type=conv_norm_type)\n",
    "\n",
    "            if i == self.num_stages - 1 and not attn_final_dilation:\n",
    "                attn_dw_dilation = [1, 2, 1]\n",
    "            blocks = nn.ModuleList([\n",
    "                MogaBlock(\n",
    "                    embed_dims=self.embed_dims[i],\n",
    "                    ffn_ratio=self.ffn_ratios[i],\n",
    "                    drop_rate=drop_rate,\n",
    "                    drop_path_rate=dpr[cur_block_idx + j],\n",
    "                    norm_type=conv_norm_type,\n",
    "                    init_value=init_value,\n",
    "                    attn_dw_dilation=attn_dw_dilation,\n",
    "                    attn_channel_split=attn_channel_split,\n",
    "                    attn_act_type=attn_act_type,\n",
    "                    attn_force_fp32=attn_force_fp32,\n",
    "                ) for j in range(depth)\n",
    "            ])\n",
    "            cur_block_idx += depth\n",
    "            norm = build_norm_layer(stem_norm_type, self.embed_dims[i])\n",
    "\n",
    "            self.add_module(f'patch_embed{i + 1}', patch_embed)\n",
    "            self.add_module(f'blocks{i + 1}', blocks)\n",
    "            self.add_module(f'norm{i + 1}', norm)\n",
    "\n",
    "        if self.fork_feat:\n",
    "            self.head = nn.Identity()\n",
    "        else:\n",
    "            # Classifier head\n",
    "            self.num_classes = num_classes\n",
    "            self.head = nn.Linear(self.embed_dims[-1], num_classes) \\\n",
    "                if num_classes > 0 else nn.Identity()\n",
    "\n",
    "            # init for classification\n",
    "            self.apply(self._init_weights)\n",
    "            self.head.weight.data.mul_(head_init_scale)\n",
    "            self.head.bias.data.mul_(head_init_scale)\n",
    "\n",
    "        self.init_cfg = copy.deepcopy(init_cfg)\n",
    "        # load pre-trained model \n",
    "        if self.fork_feat and (\n",
    "                self.init_cfg is not None or pretrained is not None):\n",
    "            self.init_weights(pretrained)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        \"\"\" Init for timm image classification \"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.BatchNorm2d, nn.LayerNorm)):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        \"\"\" Init for mmdetection or mmsegmentation by loading pre-trained weights \"\"\"\n",
    "        logger = get_root_logger()\n",
    "        if self.init_cfg is None and pretrained is None:\n",
    "            logger.warn(f'No pre-trained weights for '\n",
    "                        f'{self.__class__.__name__}, '\n",
    "                        f'training start from scratch')\n",
    "            pass\n",
    "        else:\n",
    "            if self.init_cfg is not None:\n",
    "                assert 'checkpoint' in self.init_cfg, f'Only support specify ' \\\n",
    "                                                      f'`Pretrained` in `init_cfg` in ' \\\n",
    "                                                      f'{self.__class__.__name__} '\n",
    "                ckpt_path = self.init_cfg['checkpoint']\n",
    "            elif pretrained is not None:\n",
    "                ckpt_path = pretrained\n",
    "\n",
    "            ckpt = _load_checkpoint(ckpt_path, logger=logger, map_location='cpu')\n",
    "            if 'state_dict' in ckpt:\n",
    "                _state_dict = ckpt['state_dict']\n",
    "            elif 'model' in ckpt:\n",
    "                _state_dict = ckpt['model']\n",
    "            else:\n",
    "                _state_dict = ckpt\n",
    "\n",
    "            state_dict = _state_dict\n",
    "            missing_keys, unexpected_keys = \\\n",
    "                self.load_state_dict(state_dict, False)\n",
    "            # show for debug\n",
    "            # print('missing_keys: ', missing_keys)\n",
    "            # print('unexpected_keys: ', unexpected_keys)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        outs = []\n",
    "        for i in range(self.num_stages):\n",
    "            patch_embed = getattr(self, f'patch_embed{i + 1}')\n",
    "            blocks = getattr(self, f'blocks{i + 1}')\n",
    "            norm = getattr(self, f'norm{i + 1}')\n",
    "\n",
    "            x, hw_shape = patch_embed(x)\n",
    "            for block in blocks:\n",
    "                x = block(x)\n",
    "            if self.use_layer_norm:\n",
    "                x = x.flatten(2).transpose(1, 2)\n",
    "                x = norm(x)\n",
    "                x = x.reshape(-1, *hw_shape,\n",
    "                              blocks.out_channels).permute(0, 3, 1, 2).contiguous()\n",
    "            else:\n",
    "                x = norm(x)\n",
    "            if self.fork_feat:\n",
    "                outs.append(x)\n",
    "\n",
    "        if self.fork_feat:\n",
    "            # output the features of four stages for dense prediction\n",
    "            return outs\n",
    "        else:\n",
    "            # output only the last layer for image classification\n",
    "            return x\n",
    "\n",
    "    def forward_head(self, x):\n",
    "        return self.head(x.mean(dim=[2, 3]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        if self.fork_feat:\n",
    "            # for dense prediction\n",
    "            return x\n",
    "        else:\n",
    "            # for image classification\n",
    "            return self.forward_head(x)\n",
    "\n",
    "\n",
    "input_tensor = torch.randn(8, 3, 224, 224)  # Batch of 8, 3x224x224 images\n",
    "model = MogaNet()\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  # Expected output shape: (8, 1000)\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdl",
   "language": "python",
   "name": "bdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
