{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66ba5e6",
   "metadata": {},
   "source": [
    "# Conditional Random Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57444085",
   "metadata": {},
   "source": [
    "Conditional Random Fields (CRFs) are an undirected graph model used for sequence annotation and structured prediction tasks. Compared with Hidden Markov Models (HMMs), CRFs are more suitable for modeling non local dependencies between sequences. In NLP, CRFs are commonly used for tasks such as named entity recognition, part of speech tagging, and syntactic analysis.\n",
    "\n",
    "The core idea of CRFs is the conditional probability distribution of the output sequence under given input sequence conditions. It trains the model by maximizing the conditional log likelihood function, typically using optimization algorithms such as gradient descent for parameter estimation.\n",
    "\n",
    "CRFs have similarities with Hidden Markov Models (HMMs), but they differ in several key aspects:\n",
    "\n",
    "**Label dependency**:\n",
    "\n",
    "- HMMs typically assume that the transition between labels (or hidden states) only depends on the previous state, meaning they have Markov properties.\n",
    "\n",
    "- CRFs allow the transition between labels to depend on the entire input sequence, rather than solely on the previous state. This means that CRFs can capture dependencies over longer distances.\n",
    "\n",
    "**Probability calculation**:\n",
    "\n",
    "- In HMMs, the label probabilities of sequences are calculated through multiplication, as they assume that state transitions only depend on the previous state.\n",
    "\n",
    "- CRFs use a global normalization factor (also known as a partition function) that considers all possible label sequences to calculate the observation probability of a given input sequence.\n",
    "\n",
    "**Training and inference**:\n",
    "\n",
    "- The training and label inference of HMMs can usually be efficiently completed using dynamic programming algorithms such as the Viterbi algorithm.\n",
    "\n",
    "- The training and inference of CRFs are usually more complex as they require optimization of the entire label sequence. This usually involves iterative algorithms such as gradient descent or Newton's method.\n",
    "\n",
    "The main components of CRF include:\n",
    "\n",
    "`Characteristic function`: defines the relationship between input sequence and label sequence. CRFs capture the relationship between input and output through feature functions.\n",
    "\n",
    "`Weight`: The parameter associated with the feature function, learned through training data.\n",
    "\n",
    "`Partition function`: a normalization factor used to ensure that the sum of probabilities of all possible label sequences is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "161c5353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('John', 'NNP'), ('is', 'VBZ'), ('from', 'IN'), ('New', 'NNP'), ('York', 'NNP')], [('Alice', 'NNP'), ('loves', 'VBZ'), ('to', 'TO'), ('read', 'VB'), ('books', 'NNS')], [('The', 'DT'), ('Eiffel', 'NNP'), ('Tower', 'NNP'), ('is', 'VBZ'), ('in', 'IN'), ('Paris', 'NNP')], [('Microsoft', 'NNP'), ('released', 'VBD'), ('a', 'DT'), ('new', 'JJ'), ('product', 'NN')], [('He', 'PRP'), ('visited', 'VBD'), ('the', 'DT'), ('Great', 'NNP'), ('Wall', 'NNP'), ('of', 'IN'), ('China', 'NNP')], [('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')], [('Artificial', 'JJ'), ('Intelligence', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('future', 'NN')], [('Python', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('programming', 'NN'), ('language', 'NN')]]\n",
      "[[('Jane', 'NNP'), ('works', 'VBZ'), ('at', 'IN'), ('Google', 'NNP')], [('They', 'PRP'), ('traveled', 'VBD'), ('to', 'TO'), ('San', 'NNP'), ('Francisco', 'NNP')], [('He', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('good', 'JJ'), ('person', 'NN')], [('The', 'DT'), ('new', 'JJ'), ('phone', 'NN'), ('was', 'VBD'), ('released', 'VBN'), ('yesterday', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "train_sents = [\n",
    "    [('John', 'NNP'), ('is', 'VBZ'), ('from', 'IN'), ('New', 'NNP'), ('York', 'NNP')],\n",
    "    [('Alice', 'NNP'), ('loves', 'VBZ'), ('to', 'TO'), ('read', 'VB'), ('books', 'NNS')],\n",
    "    [('The', 'DT'), ('Eiffel', 'NNP'), ('Tower', 'NNP'), ('is', 'VBZ'), ('in', 'IN'), ('Paris', 'NNP')],\n",
    "    [('Microsoft', 'NNP'), ('released', 'VBD'), ('a', 'DT'), ('new', 'JJ'), ('product', 'NN')],\n",
    "    [('He', 'PRP'), ('visited', 'VBD'), ('the', 'DT'), ('Great', 'NNP'), ('Wall', 'NNP'), ('of', 'IN'), ('China', 'NNP')],\n",
    "    [('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')],\n",
    "    [('Artificial', 'JJ'), ('Intelligence', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('future', 'NN')],\n",
    "    [('Python', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('programming', 'NN'), ('language', 'NN')]\n",
    "]\n",
    "\n",
    "test_sents = [\n",
    "    [('Jane', 'NNP'), ('works', 'VBZ'), ('at', 'IN'), ('Google', 'NNP')],\n",
    "    [('They', 'PRP'), ('traveled', 'VBD'), ('to', 'TO'), ('San', 'NNP'), ('Francisco', 'NNP')],\n",
    "    [('He', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('good', 'JJ'), ('person', 'NN')],\n",
    "    [('The', 'DT'), ('new', 'JJ'), ('phone', 'NN'), ('was', 'VBD'), ('released', 'VBN'), ('yesterday', 'NN')]\n",
    "]\n",
    "\n",
    "print(train_sents)\n",
    "print(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c930d2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['Jane', 'works', 'at', 'Google']\n",
      "True POS tags: ['NNP', 'VBZ', 'IN', 'NNP']\n",
      "Predicted POS tags: ['NNP', 'VBZ', 'IN', 'NNP']\n",
      "\n",
      "Sentence: ['They', 'traveled', 'to', 'San', 'Francisco']\n",
      "True POS tags: ['PRP', 'VBD', 'TO', 'NNP', 'NNP']\n",
      "Predicted POS tags: ['NNP', 'VBD', 'DT', 'NNP', 'NNP']\n",
      "\n",
      "Sentence: ['He', 'is', 'a', 'good', 'person']\n",
      "True POS tags: ['PRP', 'VBZ', 'DT', 'JJ', 'NN']\n",
      "Predicted POS tags: ['NNP', 'VBZ', 'DT', 'JJ', 'NN']\n",
      "\n",
      "Sentence: ['The', 'new', 'phone', 'was', 'released', 'yesterday']\n",
      "True POS tags: ['DT', 'JJ', 'NN', 'VBD', 'VBN', 'NN']\n",
      "Predicted POS tags: ['DT', 'JJ', 'NN', 'VBD', 'VBD', 'NN']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pycrfsuite\n",
    "\n",
    "# Characteristic function\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word[-3:]=' + word[-3:],\n",
    "        'word[-2:]=' + word[-2:],\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.istitle=%s' % word.istitle(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "        'postag=' + postag,\n",
    "    ]\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:postag=' + postag1,\n",
    "            '-1:word.istitle=%s' % word1.istitle(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            '+1:postag=' + postag1,\n",
    "            '+1:word.istitle=%s' % word1.istitle(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "    return features\n",
    "\n",
    "# labels\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [postag for token, postag in sent]\n",
    "\n",
    "# train data\n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "# test data\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]\n",
    "\n",
    "# train CRF\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)\n",
    "trainer.set_params({\n",
    "    'c1': 1.0,\n",
    "    'c2': 1e-3,\n",
    "    'max_iterations': 50,\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "trainer.train('example.crfsuite')\n",
    "\n",
    "# predict\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('example.crfsuite')\n",
    "y_pred = [tagger.tag(xseq) for xseq in X_test]\n",
    "\n",
    "for sent, true_labels, pred_labels in zip(test_sents, y_test, y_pred):\n",
    "    print(\"Sentence:\", [token for token, postag in sent])\n",
    "    print(\"True POS tags:\", true_labels)\n",
    "    print(\"Predicted POS tags:\", pred_labels)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdl",
   "language": "python",
   "name": "bdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
