{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c21f7e2-9e90-457e-a96e-aa5504fc60bc",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "\n",
    "Before introducing the recommended optimizer, it is important to clarify that no optimizer is the 'best' optimizer suitable for all types of machine learning problems and model architectures. Even comparing the performance of optimizers is a daunting task.\n",
    "\n",
    "We suggest sticking to using mature and popular optimizers, and ideally, choosing the most commonly used optimizer for similar problems. Common and well-established optimizers include (but are not limited to):\n",
    "- **SGD with momentum**\n",
    "- **Adam and NAdam**, They are more versatile than SGDs with momentum. Please note that Adam has 4 adjustable hyperparameters, all of which are important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc05da1-0b92-4dc2-9fb5-6ee7935e8ef2",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "### Concept\n",
    "Stochastic Gradient Descent (SGD) is an optimization algorithm that updates the model's parameters by computing the gradient of the loss function with respect to the parameters for each training sample and adjusting the parameters in the opposite direction of the gradient. This helps in finding the minimum of the loss function. SGD with Momentum is an extension of SGD that helps accelerate gradients vectors in the right directions, thus leading to faster converging. Momentum is added to the gradient update to prevent oscillations and improve convergence speed.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "v &= \\gamma v + \\eta \\nabla J(w) \\\\\n",
    "w &= w - v\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "$$\n",
    "\\begin{align*}\n",
    "v &\\text{ is the velocity} \\\\\n",
    "\\gamma &\\text{ is the momentum term} \\\\\n",
    "\\eta &\\text{ is the learning rate} \\\\\n",
    "\\nabla J(w) &\\text{ is the gradient of the loss function with respect to the weights}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Parameters\n",
    "- `params`: Iterable of parameters to optimize or dictionaries defining parameter groups.\n",
    "- `lr`: Learning rate (default: 0.01).\n",
    "- `momentum`: Momentum factor (default: 0).\n",
    "- `dampening`: Dampening for momentum (default: 0).\n",
    "- `weight_decay`: Weight decay (L2 penalty) (default: 0).\n",
    "- `nesterov`: Enables Nesterov momentum (default: False).\n",
    "\n",
    "### PyTorch Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0676090a-e830-4b61-b5e5-baed83421b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "sgd = optim.SGD(model.parameters(), lr=0.01)\n",
    "sgd_with_momentum = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d466a35-b377-4de1-85ea-a1f0b9734b60",
   "metadata": {},
   "source": [
    "## Adam (Adaptive Moment Estimation)\n",
    "\n",
    "### Concept\n",
    "Adam is an adaptive learning rate optimization algorithm designed to combine the advantages of both the AdaGrad and RMSProp algorithms. It computes adaptive learning rates for each parameter by considering the first moment (mean) and the second moment (uncentered variance) of the gradients.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "m &= \\beta_1 m + (1 - \\beta_1) \\nabla J(w) \\\\\n",
    "v &= \\beta_2 v + (1 - \\beta_2) (\\nabla J(w))^2 \\\\\n",
    "w &= w - \\eta \\frac{m}{\\sqrt{v} + \\epsilon}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "$$\n",
    "\\begin{align*}\n",
    "m &\\text{ is the estimate of the first moment (the mean of the gradients)} \\\\\n",
    "v &\\text{ is the estimate of the second moment (the mean of the squared gradients)} \\\\\n",
    "\\beta_1 &\\text{ and } \\beta_2 \\text{ are the exponential decay rates for the first and second moment estimates, respectively} \\\\\n",
    "\\epsilon &\\text{ is a small constant for numerical stability}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Parameters\n",
    "- `params`: Iterable of parameters to optimize or dictionaries defining parameter groups.\n",
    "- `lr`: Learning rate (default: 0.001).\n",
    "- `betas`: Coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)).\n",
    "- `eps`: Term added to the denominator to improve numerical stability (default: 1e-8).\n",
    "- `weight_decay`: Weight decay (L2 penalty) (default: 0).\n",
    "- `amsgrad`: Whether to use the AMSGrad variant of this algorithm (default: False).\n",
    "\n",
    "### PyTorch Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e507e9c6-6565-4e21-8cd2-76be7f50ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e226ec4c-fdf7-4729-a826-ca73cd2a0e9c",
   "metadata": {},
   "source": [
    "## NAdam (Nesterov-accelerated Adaptive Moment Estimation)\n",
    "\n",
    "### Concept\n",
    "NAdam is a combination of Adam and Nesterov momentum. It incorporates the Nesterov momentum into the Adam optimizer to achieve a faster convergence.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "g_t &= \\nabla J(w_t) \\\\\n",
    "m &= \\beta_1 m + (1 - \\beta_1) g_t \\\\\n",
    "v &= \\beta_2 v + (1 - \\beta_2) g_t^2 \\\\\n",
    "\\hat{m} &= m / (1 - \\beta_1^t) \\\\\n",
    "\\hat{v} &= v / (1 - \\beta_2^t) \\\\\n",
    "w_{t+1} &= w_t - \\eta \\frac{\\hat{m} + \\epsilon}{\\sqrt{\\hat{v}} + \\epsilon}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "$$\n",
    "\\begin{align*}\n",
    "g_t &\\text{ is the gradient at time } t \\\\\n",
    "\\hat{m} &\\text{ and } \\hat{v} \\text{ are the unbiased estimates of the first and second moment estimates, respectively}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Parameters\n",
    "- `params`: Iterable of parameters to optimize or dictionaries defining parameter groups.\n",
    "- `lr`: Learning rate (default: 0.001).\n",
    "- `betas`: Coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)).\n",
    "- `eps`: Term added to the denominator to improve numerical stability (default: 1e-8).\n",
    "- `weight_decay`: Weight decay (L2 penalty) (default: 0).\n",
    "- `momentum_decay`: Momentum decay term (default: 4e-3).\n",
    "\n",
    "### PyTorch Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c20771a-3d3e-4c78-9279-e007d86cf183",
   "metadata": {},
   "outputs": [],
   "source": [
    "nadam = optim.NAdam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78264d2e-8837-4978-ad14-e25de4dd3919",
   "metadata": {},
   "source": [
    "# Batch Size\n",
    "\n",
    "Batch Size is a key factor in determining training time and computational resource consumption.<br>\n",
    "\n",
    "Increasing Batch Size usually reduces training time. This is very beneficial because it:<br>\n",
    "- It can make hyperparameter adjustment more thorough within a fixed time interval, and ultimately train a better model.\n",
    "- Reducing development cycle latency allows for more testing of new ideas.\n",
    "\n",
    "There is no clear relationship between resource consumption and Batch Size. Increasing Batch Size can increase, decrease, or maintain resource consumption unchanged possible.<br>\n",
    "Batch Size should not be used as a tunable hyperparameter for validation set performance. As long as all hyperparameters (especially learning rate and regularization hyperparameters) are adjusted well and the training steps are sufficient, theoretically any batch size can achieve the same final performance.\n",
    "\n",
    "For a given model and optimizer, available hardware can typically support a range of Batch Sizes. The limiting factor is usually the memory of accelerators (GPUs/TPUs, etc.). Unfortunately, it is difficult to calculate a suitable batch size for memory without running or compiling a complete training program. The simplest solution is usually to run a small number of training experiments with different batch sizes (for example, trying to power 2) until one of the experiments exceeds the available memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdl",
   "language": "python",
   "name": "bdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
