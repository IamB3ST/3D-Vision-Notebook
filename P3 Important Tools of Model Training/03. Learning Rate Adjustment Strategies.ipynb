{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00167a2a-97aa-4e89-8a45-51da9b333cf7",
   "metadata": {},
   "source": [
    "# Learning Rate Adjustment Strategies\n",
    "\n",
    "In deep learning, the learning rate is a crucial hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. Proper adjustment of the learning rate can significantly improve model performance and convergence speed. This blog introduces various learning rate adjustment strategies, including fixed learning rate, step decay, exponential decay, cosine annealing, cyclical learning rate, adaptive learning rate, learning rate warm-up, and learning rate finder, along with their application scenarios.\n",
    "\n",
    "## 1. Fixed Learning Rate\n",
    "\n",
    "### Description\n",
    "A fixed learning rate means keeping the learning rate constant throughout the training process. This is the simplest strategy and can be effective for simpler problems or when computational resources are limited.\n",
    "\n",
    "### Application\n",
    "- Suitable for small datasets or shallow networks.\n",
    "- Used when training time is limited, and a quick solution is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5111f3d3-df57-402e-adf7-35141957e7b7",
   "metadata": {},
   "source": [
    "## 2. Step Decay\n",
    "\n",
    "### Description\n",
    "Step decay reduces the learning rate by a factor at specific intervals. This approach helps in maintaining a high learning rate at the beginning and reducing it as the training progresses.\n",
    "\n",
    "### Application\n",
    "- Effective in scenarios where a significant drop in the learning rate is required after certain epochs.\n",
    "- Commonly used in computer vision tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1d330d-59aa-46e9-a355-81f45acce65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Define step decay scheduler\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587823c7-2208-4fb4-bb54-149355c3a3a7",
   "metadata": {},
   "source": [
    "## 3. Exponential Decay\n",
    "\n",
    "### Description\n",
    "Exponential decay decreases the learning rate exponentially over time. This allows for a smoother reduction compared to step decay.\n",
    "\n",
    "### Application\n",
    "- Useful for large-scale models where a gradual reduction in learning rate can stabilize training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25654464-6128-49a5-8655-96acbb62cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Define exponential decay scheduler\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae9a58f-63ae-4da5-a424-f143930d613a",
   "metadata": {},
   "source": [
    "## 4. Cosine Annealing\n",
    "\n",
    "### Description\n",
    "Cosine annealing reduces the learning rate following a cosine curve. This method can potentially escape local minima by allowing the learning rate to increase again.\n",
    "\n",
    "### Application\n",
    "- Effective in training neural networks for image and natural language processing tasks.\n",
    "- Used in conjunction with warm restarts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b517f91-aa7f-45a0-8f9b-dbc4349df7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Define cosine annealing scheduler\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a1ca48-a13a-47c0-81f1-01665644d111",
   "metadata": {},
   "source": [
    "## 5. Cyclical Learning Rate\n",
    "\n",
    "### Description\n",
    "Cyclical learning rate (CLR) oscillates between a lower and upper bound, rather than decreasing monotonically. This can help the model escape local minima.\n",
    "\n",
    "### Application\n",
    "- Beneficial in training neural networks where the landscape of the loss function is complex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d08a9e-dab1-4af1-9e32-5386b36dbc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define cyclical learning rate scheduler\n",
    "scheduler = CyclicLR(optimizer, base_lr=0.001, max_lr=0.006, step_size_up=2000, mode='triangular')\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071422cc-0b68-4f3c-b2cc-d8c47a26a8e0",
   "metadata": {},
   "source": [
    "## 6. Adaptive Learning Rate\n",
    "\n",
    "### Description\n",
    "Adaptive learning rates, used in optimizers like AdaGrad, RMSprop, and Adam, adjust the learning rate based on past gradient information.\n",
    "\n",
    "### Application\n",
    "- Suitable for complex problems with sparse gradients.\n",
    "- Common in natural language processing and recommendation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811167f5-f4f3-42bc-91e2-86e14b07cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam, AdaGrad, and RMSprop are adaptive learning rate optimizers\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ad323a-cb3c-4624-aa57-b7f5963aebd1",
   "metadata": {},
   "source": [
    "## 7. Learning Rate Warm-Up\n",
    "\n",
    "### Description\n",
    "Learning rate warm-up gradually increases the learning rate from a small value to the desired value. This can help stabilize the training process, especially in the initial phase.\n",
    "\n",
    "### Application\n",
    "- Often used in conjunction with other learning rate schedules like step decay or cosine annealing.\n",
    "- Effective in training deep networks such as transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66d5d4d-2207-416c-ad77-0988a07927cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define learning rate warm-up scheduler\n",
    "def lr_lambda(epoch):\n",
    "    warmup_epochs = 5\n",
    "    if epoch < warmup_epochs:\n",
    "        return (epoch + 1) / warmup_epochs\n",
    "    else:\n",
    "        return 0.1 ** ((epoch - warmup_epochs) / (num_epochs - warmup_epochs))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e939aa-55a5-4270-8fc0-c82947730587",
   "metadata": {},
   "source": [
    "## 8. Learning Rate Finder\n",
    "\n",
    "### Description\n",
    "Learning rate finder helps to find the optimal learning rate by running a short training cycle with exponentially increasing learning rates and plotting the loss.\n",
    "\n",
    "### Application\n",
    "- Useful for setting an appropriate initial learning rate before full training.\n",
    "- Can be applied across various domains and models to tune learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14549f5-c6a3-4dc4-92a4-651a085ae8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-7)\n",
    "\n",
    "# Define learning rate finder\n",
    "class LRFinder:\n",
    "    def __init__(self, model, optimizer, criterion, train_loader):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.train_loader = train_loader\n",
    "        self.history = {\"lr\": [], \"loss\": []}\n",
    "\n",
    "    def find(self, start_lr=1e-7, end_lr=10, num_it=100):\n",
    "        self.optimizer.param_groups[0]['lr'] = start_lr\n",
    "        lr_mult = (end_lr / start_lr) ** (1 / num_it)\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(self.train_loader):\n",
    "            if batch_idx > num_it:\n",
    "                break\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            lr = self.optimizer.param_groups[0]['lr']\n",
    "            self.history[\"lr\"].append(lr)\n",
    "            self.history[\"loss\"].append(loss.item())\n",
    "\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "            if loss.item() > 4 * best_loss:\n",
    "                break\n",
    "\n",
    "            lr *= lr_mult\n",
    "            self.optimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "    def plot(self):\n",
    "        plt.plot(self.history[\"lr\"], self.history[\"loss\"])\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel(\"Learning Rate\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "\n",
    "# Instantiate learning rate finder\n",
    "lr_finder = LRFinder(model, optimizer, criterion, train_loader)\n",
    "\n",
    "# Find and plot learning rates\n",
    "lr_finder.find()\n",
    "lr_finder.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdl",
   "language": "python",
   "name": "bdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
