{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68932fef-1256-4a36-948b-5dbdc42adfa4",
   "metadata": {},
   "source": [
    "# Label Smoothing\n",
    "\n",
    "Label smoothing is a regularization technique used to improve the performance and generalization of deep learning models, particularly in classification tasks. By altering the hard labels during training, label smoothing helps mitigate issues such as overfitting and overconfidence in the model's predictions.\n",
    "\n",
    "## What is Label Smoothing?\n",
    "\n",
    "In traditional classification tasks, the ground truth labels are represented as one-hot vectors. For instance, if there are three classes and the true label is class 2, the one-hot vector would be `[0, 1, 0]`. Label smoothing, on the other hand, replaces the hard 0 and 1 values with softer values. \n",
    "\n",
    "For example, with a smoothing factor alpha, the smoothed label y smooth for a true label y true can be computed as:\n",
    "\n",
    "$$\n",
    "y_{smooth} = (1 - \\alpha) \\cdot y_{true} + \\alpha \\cdot u\n",
    "$$\n",
    "\n",
    "This results in a label vector that looks something like `[0.1, 0.8, 0.1]` instead of `[0, 1, 0]` for alpha = 0.2.\n",
    "\n",
    "## Why Use Label Smoothing?\n",
    "\n",
    "### 1. Reducing Overfitting\n",
    "\n",
    "Label smoothing introduces a small amount of noise into the labels, which acts as a form of regularization. This helps prevent the model from becoming too confident in its predictions, thereby reducing overfitting on the training data.\n",
    "\n",
    "### 2. Mitigating Overconfidence\n",
    "\n",
    "Neural networks can become overconfident in their predictions, assigning probabilities very close to 1 for the predicted class. This overconfidence can be problematic, especially when the model encounters out-of-distribution data. Label smoothing encourages the model to be less certain, leading to better calibration of the predicted probabilities.\n",
    "\n",
    "### 3. Improving Generalization\n",
    "\n",
    "By smoothing the labels, the model learns to distribute some probability mass to all classes, rather than focusing entirely on the correct class. This can lead to improved generalization performance on unseen data.\n",
    "\n",
    "## Implementing Label Smoothing\n",
    "\n",
    "Label smoothing can be easily implemented in most deep learning frameworks. Below is an example implementation using Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b91a5a-4249-4daf-a58b-f6bf9ae8b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LabelSmoothCEloss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,  pred,  label,  smoothing=0.1):\n",
    "        pred = F.softmax(pred,  dim=1)\n",
    "        one_hot_label = F.one_hot(label, pred.size(1)).float()\n",
    "        smoothed_one_hot_label = (1.0 - smoothing)  *  one_hot_label + smoothing / pred.size(1)\n",
    "        loss = (-torch.log(pred))  *  smoothed_one_hot_label\n",
    "        loss = loss.sum(axis=1,  keepdim=False)\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "criterion = LabelSmoothCEloss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40aa97f-5235-4c9b-ab82-fef10316f3fb",
   "metadata": {},
   "source": [
    "# Augmentation\n",
    "We have introduced some useful data augmentation techniques for images in P1 chapter4. In this chapter, we will introduce two important techniques that we did not mention in previous chapters. They are `Mixup` and `Cutmix`.\n",
    "\n",
    "## Mixup\n",
    "\n",
    "Mixup is a simple yet effective data augmentation technique, particularly useful for image classification tasks. Its basic idea is to perform linear interpolation between the images and their labels in the input data. Specifically, for each pair of training samples, a new sample is created by taking a weighted average of two samples. This weighted average process is performed not only on the input images but also on the corresponding labels.\n",
    "\n",
    "For example, given two training samples (x_1, y_1) and (x_2, y_2), where x_1 and x_2 are images, and y_1 and y_2 are the corresponding labels, the new sample generated by Mixup is as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\tilde{x} &= \\lambda x_1 + (1 - \\lambda) x_2 \\\\\n",
    "\\tilde{y} &= \\lambda y_1 + (1 - \\lambda) y_2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\tilde{x} &\\text{ is the new image} \\\\\n",
    "\\tilde{y} &\\text{ is the new label} \\\\\n",
    "\\lambda &\\text{ is a random interpolation coefficient sampled from a beta distribution.} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Mixup encourages the model to learn more generalizable features by blending different samples together, thereby reducing overfitting and improving the model's performance on unseen data.\n",
    "\n",
    "## Benefits of Mixup:\n",
    "\n",
    "1. **Regularization**: Mixup acts as a form of regularization by adding noise to the training data, which helps prevent the model from overfitting.\n",
    "\n",
    "2. **Improving Generalization**: By blending samples, Mixup encourages the model to learn more robust features that generalize better to unseen data.\n",
    "\n",
    "3. **Enhanced Diversity**: Mixup increases the diversity of the training data by creating new samples from combinations of existing ones, leading to a more comprehensive learning process.\n",
    "\n",
    "## Implementation:\n",
    "\n",
    "Mixup can be easily implemented in most deep learning frameworks. Below is a simple example of how to implement Mixup in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf62eda-a338-49fd-93db-5fce85469bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(images,target) in enumerate(train_loader):\n",
    "    # 1.input output\n",
    "    images = images.cuda(non_blocking=True)\n",
    "    target = torch.from_numpy(np.array(target)).float().cuda(non_blocking=True)\n",
    "\n",
    "    # 2.mixup\n",
    "    alpha=config.alpha\n",
    "    lam = np.random.beta(alpha,alpha)\n",
    "    index = torch.randperm(images.size(0)).cuda()\n",
    "    inputs = lam*images + (1-lam)*images[index,:]\n",
    "    targets_a, targets_b = target, target[index]\n",
    "    outputs = model(inputs)\n",
    "    loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
    "\n",
    "    # 3.backward\n",
    "    optimizer.zero_grad()   # reset gradient\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d4948d-2e18-4c08-b254-d9b70fbfa043",
   "metadata": {},
   "source": [
    "## CutMix\n",
    "\n",
    "CutMix is a powerful data augmentation technique designed to improve the generalization and robustness of convolutional neural networks (CNNs) in image classification tasks. It involves cutting and pasting patches from one image onto another and mixing their labels accordingly.\n",
    "\n",
    "### How CutMix Works:\n",
    "\n",
    "1. **Image Transformation**:\n",
    "   - Select a random image from the dataset.\n",
    "   - Randomly choose a rectangular patch within the image.\n",
    "   - Replace this patch with a patch from another randomly chosen image in the dataset.\n",
    "   \n",
    "2. **Label Generation**:\n",
    "   - The new image is assigned a label that is a weighted average of the original labels of the two images, based on the area of overlap.\n",
    "   \n",
    "3. **Training**:\n",
    "   - The model is trained on the augmented images with their corresponding mixed labels.\n",
    "\n",
    "### Benefits of CutMix:\n",
    "\n",
    "1. **Regularization**:\n",
    "   - CutMix acts as a form of regularization by adding noise to the training data, reducing overfitting and improving the model's generalization ability.\n",
    "   \n",
    "2. **Improved Robustness**:\n",
    "   - By combining patches from different images, CutMix encourages the model to learn features that are robust to variations in the input data.\n",
    "   \n",
    "3. **Increased Diversity**:\n",
    "   - CutMix increases the diversity of the training data by creating new samples from combinations of existing ones, leading to a more comprehensive learning process.\n",
    "\n",
    "### Implementation:\n",
    "\n",
    "CutMix can be implemented in most deep learning frameworks. Here's a high-level overview of how to implement CutMix in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d873c598-6c7d-4213-9b55-5a20943f6e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutmix(data, targets, alpha=1.0):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets = targets[indices]\n",
    "    \n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    \n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
    "    new_data = data.clone()\n",
    "    new_data[:, :, bbx1:bbx2, bby1:bby2] = shuffled_data[:, :, bbx1:bbx2, bby1:bby2]\n",
    "    \n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size(-1) * data.size(-2)))\n",
    "    \n",
    "    targets = (targets, shuffled_targets, lam)\n",
    "    return new_data, targets\n",
    "\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "\n",
    "for inputs, targets in data_loader:\n",
    "    inputs, targets = cutmix(inputs, targets)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = cutmix_criterion(criterion, outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdl",
   "language": "python",
   "name": "bdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
